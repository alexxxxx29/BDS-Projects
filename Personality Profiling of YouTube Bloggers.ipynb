{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5159b8e",
   "metadata": {
    "_cell_guid": "39fc8038-1e29-4bb3-8486-160aa629127b",
    "_execution_state": "idle",
    "_uuid": "047afe52-49c0-4445-bc38-8ce6b6825e9d",
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:25.356780Z",
     "iopub.status.busy": "2023-09-18T11:35:25.354591Z",
     "iopub.status.idle": "2023-09-18T11:35:27.998576Z",
     "shell.execute_reply": "2023-09-18T11:35:27.996545Z"
    },
    "papermill": {
     "duration": 2.670002,
     "end_time": "2023-09-18T11:35:28.002180",
     "exception": false,
     "start_time": "2023-09-18T11:35:25.332178",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.2     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.1     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mselect()\u001b[39m masks \u001b[34mMASS\u001b[39m::select()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    }
   ],
   "source": [
    "library(MASS)\n",
    "library(tidyverse) \n",
    "library(tidytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761ebaa4",
   "metadata": {
    "_cell_guid": "60fffa58-51df-4b85-8405-7208ac965554",
    "_uuid": "5a7e0e4d-bd75-4313-ad70-0df228feef48",
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:28.087768Z",
     "iopub.status.busy": "2023-09-18T11:35:28.042752Z",
     "iopub.status.idle": "2023-09-18T11:35:28.125095Z",
     "shell.execute_reply": "2023-09-18T11:35:28.122898Z"
    },
    "papermill": {
     "duration": 0.108254,
     "end_time": "2023-09-18T11:35:28.129342",
     "exception": false,
     "start_time": "2023-09-18T11:35:28.021088",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "master_dir = file.path(list.files('../input', full.names=TRUE), 'youtube-personality')\n",
    "directory_content = list.files(master_dir, full.names = TRUE)\n",
    "\n",
    "# Path to the transcripts directory with transcript .txt files\n",
    "path_to_transcripts = directory_content[2] \n",
    "\n",
    "# .csv filenames (see output above)\n",
    "AudioVisual_file    = directory_content[3]\n",
    "Gender_file         = directory_content[4]\n",
    "Personality_file    = directory_content[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b6ffc",
   "metadata": {
    "_cell_guid": "1e2591f6-6257-49b6-b8ca-5211d63b2c15",
    "_uuid": "5564aff0-1b9a-4ad0-8deb-c59822efb02f",
    "papermill": {
     "duration": 0.018613,
     "end_time": "2023-09-18T11:35:28.167376",
     "exception": false,
     "start_time": "2023-09-18T11:35:28.148763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Importing the data\n",
    "\n",
    "We will import the following:\n",
    "\n",
    "- Transcripts\n",
    "- Audiovisual information\n",
    "- Personality scores\n",
    "- Gender\n",
    "\n",
    "## 1.1 Description of the YouTube Personality Dataset\n",
    "\n",
    "The YouTube Personality dataset comprises an array of behavioral features, speech transcriptions, and personality impression scores for a group of 404 YouTube vloggers. These vloggers present themselves in front of a webcam while discussing a wide range of topics, encompassing personal matters, politics, movies, books, and more. Notably, there are no content-related restrictions, and the language employed in the videos is both natural and diverse.\n",
    "\n",
    "\n",
    "## 1.2 Objective: Predicting Missing Personality Trait Scores\n",
    "\n",
    "Our primary objective centers around a subset of this dataset: specifically, 80 out of the 404 vloggers have incomplete personality trait scores. Leveraging text analysis techniques, we aim to predict and fill in these missing personality scores, contributing to a more comprehensive understanding of the vloggers' personalities.\n",
    "\n",
    "## 1.3 Big Five Personality Traits \n",
    "\n",
    "### Extraversion:\n",
    "Extraversion is characterized by a high level of energy in social interactions, assertiveness, and a propensity for seeking new sensations. People with high extraversion are often described as \"life of the party,\" talkative, and natural leaders within a group.\n",
    "\n",
    "### Agreeableness:\n",
    "Agreeableness reflects an individual's desire to maintain harmonious relationships with others. Key attributes include humility, trustworthiness, and patience. Those high in agreeableness may be hesitant to express opinions that conflict with others.\n",
    "\n",
    "### Conscientiousness:\n",
    "Conscientious individuals exhibit discipline, a preference for organized tasks, and a strong commitment to doing what is morally right. Typical behaviors for conscientious individuals include never cheating and ensuring the completion of tasks.\n",
    "\n",
    "### Emotionality (Neuroticism):\n",
    "Emotionality, also known as Neuroticism, measures the extent to which a person experiences negative emotions and how these emotions impact their well-being. Individuals high in emotionality often grapple with depression, anxiety, or self-consciousness.\n",
    "\n",
    "### Openness to Experience:\n",
    "Openness to Experience captures a person's curiosity and interest in exploring new ideas, values, and behaviors.\n",
    "\n",
    "Source:\n",
    "Huntington, C. (n.d.). Big Five Personality Traits: Definition & Theory. Retrieved from https://www.berkeleywellbeing.com/big-five-personality-traits.html\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "## 1.4 Importing transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197e61c6",
   "metadata": {
    "_cell_guid": "83284cd6-8b82-4e44-b327-a7bb30035ea1",
    "_uuid": "9bb0363f-c1e8-4e78-95e8-240628cd1c56",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:28.208840Z",
     "iopub.status.busy": "2023-09-18T11:35:28.206858Z",
     "iopub.status.idle": "2023-09-18T11:35:28.296056Z",
     "shell.execute_reply": "2023-09-18T11:35:28.293711Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.113651,
     "end_time": "2023-09-18T11:35:28.299666",
     "exception": false,
     "start_time": "2023-09-18T11:35:28.186015",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "transcript_files = list.files(path_to_transcripts, full.names = TRUE) \n",
    "\n",
    "# Extract vlogger IDs\n",
    "vlogId = basename(transcript_files)\n",
    "vlogId = str_replace(vlogId, pattern = \".txt$\", replacement = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267a33e",
   "metadata": {
    "_cell_guid": "e5bd1c34-e28f-4b02-a1a5-c49037d8e40d",
    "_uuid": "a48356cc-bb9c-4edd-9cca-0082f5dfcdaf",
    "papermill": {
     "duration": 0.018687,
     "end_time": "2023-09-18T11:35:28.337126",
     "exception": false,
     "start_time": "2023-09-18T11:35:28.318439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To include features extracted from the transcript texts you will have to read the text from files and store them in a data frame. For this, you will need the full file paths as stored in `transcript_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771252aa",
   "metadata": {
    "_cell_guid": "1553130b-5066-4d9b-8219-9c1ca9b3ba1a",
    "_uuid": "bb66ae39-9ef8-46fb-8971-082dc333bdb8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:28.379226Z",
     "iopub.status.busy": "2023-09-18T11:35:28.377204Z",
     "iopub.status.idle": "2023-09-18T11:35:29.978565Z",
     "shell.execute_reply": "2023-09-18T11:35:29.976332Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.625686,
     "end_time": "2023-09-18T11:35:29.981613",
     "exception": false,
     "start_time": "2023-09-18T11:35:28.355927",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in readLines(.x):\n",
      "“incomplete final line found on '../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG11.txt'”\n"
     ]
    }
   ],
   "source": [
    "transcripts_df = tibble(\n",
    "    \n",
    "    # vlogId connects each transcripts to a vlogger\n",
    "    vlogId=vlogId,\n",
    "    \n",
    "    # Read the transcript text from all file and store as a string\n",
    "    TEXT = map_chr(transcript_files, ~ paste(readLines(.x), collapse = \"\\\\n\")), \n",
    "    \n",
    "    # `filename` keeps track of the specific video transcript\n",
    "    filename = transcript_files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3c379",
   "metadata": {
    "_cell_guid": "e6ae9665-5609-4491-a478-51e691e68549",
    "_uuid": "cbeff62e-1658-4d36-8343-e9a44a69b631",
    "papermill": {
     "duration": 0.019604,
     "end_time": "2023-09-18T11:35:30.021405",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.001801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.5 Import personality and gender scores\n",
    "\n",
    "Gender is a useful predictor for personality scores as previous research shows. Schmitt et al. (2008), for instance, conducted a cross-cultural analysis invovling data from 55 different nations (total sample size of 17,637 individuals). They found that women tended to report higher levels of neuroticism, extraversion, agreeableness, and conscientiousness compared to men in most of the nations studied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c84dedb",
   "metadata": {
    "_cell_guid": "3b23f306-d346-457a-86e5-e567b85fc8c0",
    "_uuid": "a4f9c3b5-7277-4b3f-838e-e51d3acd321e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:30.064527Z",
     "iopub.status.busy": "2023-09-18T11:35:30.062445Z",
     "iopub.status.idle": "2023-09-18T11:35:30.372125Z",
     "shell.execute_reply": "2023-09-18T11:35:30.370243Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.334254,
     "end_time": "2023-09-18T11:35:30.374781",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.040527",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m324\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m6\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \" \"\n",
      "\u001b[31mchr\u001b[39m (1): vlogId\n",
      "\u001b[32mdbl\u001b[39m (5): Extr, Agr, Cons, Emot, Open\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 3 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>4.9</td><td>3.7</td><td>3.6</td><td>3.2</td><td>5.5</td></tr>\n",
       "\t<tr><td>VLOG3</td><td>5.0</td><td>5.0</td><td>4.6</td><td>5.3</td><td>4.4</td></tr>\n",
       "\t<tr><td>VLOG5</td><td>5.9</td><td>5.3</td><td>5.3</td><td>5.8</td><td>5.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 3 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " vlogId & Extr & Agr & Cons & Emot & Open\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & 4.9 & 3.7 & 3.6 & 3.2 & 5.5\\\\\n",
       "\t VLOG3 & 5.0 & 5.0 & 4.6 & 5.3 & 4.4\\\\\n",
       "\t VLOG5 & 5.9 & 5.3 & 5.3 & 5.8 & 5.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 3 × 6\n",
       "\n",
       "| vlogId &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| VLOG1 | 4.9 | 3.7 | 3.6 | 3.2 | 5.5 |\n",
       "| VLOG3 | 5.0 | 5.0 | 4.6 | 5.3 | 4.4 |\n",
       "| VLOG5 | 5.9 | 5.3 | 5.3 | 5.8 | 5.5 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId Extr Agr Cons Emot Open\n",
       "1 VLOG1  4.9  3.7 3.6  3.2  5.5 \n",
       "2 VLOG3  5.0  5.0 4.6  5.3  4.4 \n",
       "3 VLOG5  5.9  5.3 5.3  5.8  5.5 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the Personality scores\n",
    "pers_df = read_delim(Personality_file, delim = \" \")\n",
    "\n",
    "head(pers_df, n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efacef",
   "metadata": {
    "_cell_guid": "0f1b8dec-eccb-4fe8-964d-f6ee2a2bfa2e",
    "_uuid": "483309a5-f1d9-4c52-99f6-7981af701b15",
    "papermill": {
     "duration": 0.019729,
     "end_time": "2023-09-18T11:35:30.413863",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.394134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gender info is stored in a separate `.csv` which is also delimited with a space. This file doesn't have column names, so we have to add them ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cded066",
   "metadata": {
    "_cell_guid": "4addc5a4-5b6f-4fbe-a84f-7ee5733a8271",
    "_uuid": "42642c23-d42f-4265-afd8-e03477f0ec07",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:30.456473Z",
     "iopub.status.busy": "2023-09-18T11:35:30.454810Z",
     "iopub.status.idle": "2023-09-18T11:35:30.490686Z",
     "shell.execute_reply": "2023-09-18T11:35:30.488225Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.060672,
     "end_time": "2023-09-18T11:35:30.493839",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.433167",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 3 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>gender</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG3</td><td>Female</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG5</td><td>Male  </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG6</td><td>Male  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 3 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & vlogId & gender\\\\\n",
       "  & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG3 & Female\\\\\n",
       "\t2 & VLOG5 & Male  \\\\\n",
       "\t3 & VLOG6 & Male  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 3 × 2\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | gender &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| 1 | VLOG3 | Female |\n",
       "| 2 | VLOG5 | Male   |\n",
       "| 3 | VLOG6 | Male   |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId gender\n",
       "1 VLOG3  Female\n",
       "2 VLOG5  Male  \n",
       "3 VLOG6  Male  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_df = read.delim(Gender_file, head=FALSE, sep=\" \", skip = 2)\n",
    "\n",
    "# Add column names\n",
    "names(gender_df) = c('vlogId', 'gender')\n",
    "\n",
    "\n",
    "head(gender_df, n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf4f42",
   "metadata": {
    "_cell_guid": "252e73f9-ed0f-4161-a6e8-f287e6dfaddc",
    "_uuid": "4093194b-da93-4b07-996f-82c1fe810418",
    "papermill": {
     "duration": 0.019711,
     "end_time": "2023-09-18T11:35:30.533280",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.513569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.6 Merging the gender and personality dataframes\n",
    "\n",
    "Use left_join() to merge all the information in a single tidy data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca7df01",
   "metadata": {
    "_cell_guid": "15d7bd85-f197-4803-8daa-6193f7929978",
    "_uuid": "58eb0582-3c68-458e-a807-187ef63fd52a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:30.576511Z",
     "iopub.status.busy": "2023-09-18T11:35:30.574821Z",
     "iopub.status.idle": "2023-09-18T11:35:30.618331Z",
     "shell.execute_reply": "2023-09-18T11:35:30.615878Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.068565,
     "end_time": "2023-09-18T11:35:30.621498",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.552933",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>gender</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG3</td><td>Female</td><td>5.0</td><td>5.0</td><td>4.6</td><td>5.3</td><td>4.4</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG5</td><td>Male  </td><td>5.9</td><td>5.3</td><td>5.3</td><td>5.8</td><td>5.5</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG6</td><td>Male  </td><td>5.4</td><td>4.8</td><td>4.4</td><td>4.8</td><td>5.7</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG7</td><td>Male  </td><td>4.7</td><td>5.1</td><td>4.4</td><td>5.1</td><td>4.7</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG8</td><td>Female</td><td> NA</td><td> NA</td><td> NA</td><td> NA</td><td> NA</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG9</td><td>Female</td><td>5.6</td><td>5.0</td><td>4.0</td><td>4.2</td><td>4.9</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 7\n",
       "\\begin{tabular}{r|lllllll}\n",
       "  & vlogId & gender & Extr & Agr & Cons & Emot & Open\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG3 & Female & 5.0 & 5.0 & 4.6 & 5.3 & 4.4\\\\\n",
       "\t2 & VLOG5 & Male   & 5.9 & 5.3 & 5.3 & 5.8 & 5.5\\\\\n",
       "\t3 & VLOG6 & Male   & 5.4 & 4.8 & 4.4 & 4.8 & 5.7\\\\\n",
       "\t4 & VLOG7 & Male   & 4.7 & 5.1 & 4.4 & 5.1 & 4.7\\\\\n",
       "\t5 & VLOG8 & Female &  NA &  NA &  NA &  NA &  NA\\\\\n",
       "\t6 & VLOG9 & Female & 5.6 & 5.0 & 4.0 & 4.2 & 4.9\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 7\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | gender &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "| 1 | VLOG3 | Female | 5.0 | 5.0 | 4.6 | 5.3 | 4.4 |\n",
       "| 2 | VLOG5 | Male   | 5.9 | 5.3 | 5.3 | 5.8 | 5.5 |\n",
       "| 3 | VLOG6 | Male   | 5.4 | 4.8 | 4.4 | 4.8 | 5.7 |\n",
       "| 4 | VLOG7 | Male   | 4.7 | 5.1 | 4.4 | 5.1 | 4.7 |\n",
       "| 5 | VLOG8 | Female |  NA |  NA |  NA |  NA |  NA |\n",
       "| 6 | VLOG9 | Female | 5.6 | 5.0 | 4.0 | 4.2 | 4.9 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId gender Extr Agr Cons Emot Open\n",
       "1 VLOG3  Female 5.0  5.0 4.6  5.3  4.4 \n",
       "2 VLOG5  Male   5.9  5.3 5.3  5.8  5.5 \n",
       "3 VLOG6  Male   5.4  4.8 4.4  4.8  5.7 \n",
       "4 VLOG7  Male   4.7  5.1 4.4  5.1  4.7 \n",
       "5 VLOG8  Female  NA   NA  NA   NA   NA \n",
       "6 VLOG9  Female 5.6  5.0 4.0  4.2  4.9 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vlogger_df = left_join(gender_df, pers_df, by='vlogId')\n",
    "head(vlogger_df) # VLOG8 has missing personality scores: those should be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25050b",
   "metadata": {
    "_cell_guid": "5ef44461-656c-4d24-8e4f-c137159fbc16",
    "_uuid": "d99b97d2-a621-4347-bde2-39e316bdf494",
    "papermill": {
     "duration": 0.019956,
     "end_time": "2023-09-18T11:35:30.661796",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.641840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We leave the `transcripts_df` data frame seperate for now, because you will first have to extract features from the transcripts first. Once you have those features in a tidy data frame, including a `vlogId` column, you can refer to this `left_join` example to merge your features with `vlogger_df` in one single tidy data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f6455",
   "metadata": {
    "papermill": {
     "duration": 0.019813,
     "end_time": "2023-09-18T11:35:30.701778",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.681965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.7 Import audiovisual features\n",
    "Now we can import the audiovisual features from a separate .csv file using `read_delim()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe23f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:30.746272Z",
     "iopub.status.busy": "2023-09-18T11:35:30.744374Z",
     "iopub.status.idle": "2023-09-18T11:35:30.898972Z",
     "shell.execute_reply": "2023-09-18T11:35:30.897096Z"
    },
    "papermill": {
     "duration": 0.180107,
     "end_time": "2023-09-18T11:35:30.901653",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.721546",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m404\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m26\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \" \"\n",
      "\u001b[31mchr\u001b[39m  (1): vlogId\n",
      "\u001b[32mdbl\u001b[39m (25): mean.pitch, sd.pitch, mean.conf.pitch, sd.conf.pitch, mean.spec.en...\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 3 × 26</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>mean.pitch</th><th scope=col>sd.pitch</th><th scope=col>mean.conf.pitch</th><th scope=col>sd.conf.pitch</th><th scope=col>mean.spec.entropy</th><th scope=col>sd.spec.entropy</th><th scope=col>mean.val.apeak</th><th scope=col>sd.val.apeak</th><th scope=col>mean.loc.apeak</th><th scope=col>⋯</th><th scope=col>sd.d.energy</th><th scope=col>avg.voiced.seg</th><th scope=col>avg.len.seg</th><th scope=col>time.speaking</th><th scope=col>voice.rate</th><th scope=col>num.turns</th><th scope=col>hogv.entropy</th><th scope=col>hogv.median</th><th scope=col>hogv.cogR</th><th scope=col>hogv.cogC</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>178.15</td><td>0.38358</td><td>1.2526</td><td>0.45440</td><td>3.3674</td><td>0.29309</td><td>0.82192</td><td>0.12429</td><td>0.018525</td><td>⋯</td><td>0.0255970</td><td>0.18441</td><td>1.3559</td><td>0.60796</td><td>0.051389</td><td>0.44839</td><td>7.026606</td><td>0.147870</td><td>121</td><td>198</td></tr>\n",
       "\t<tr><td>VLOG3</td><td>239.32</td><td>0.36474</td><td>1.2205</td><td>0.41543</td><td>3.8150</td><td>0.17479</td><td>0.64969</td><td>0.22731</td><td>0.027022</td><td>⋯</td><td>0.0012289</td><td>0.16404</td><td>1.0272</td><td>0.51374</td><td>0.057632</td><td>0.50013</td><td>4.006787</td><td>0.008571</td><td>175</td><td>164</td></tr>\n",
       "\t<tr><td>VLOG5</td><td>173.50</td><td>0.47636</td><td>1.1678</td><td>0.50508</td><td>3.6949</td><td>0.32347</td><td>0.65878</td><td>0.22253</td><td>0.021466</td><td>⋯</td><td>0.0026112</td><td>0.30966</td><td>2.2164</td><td>0.70205</td><td>0.037614</td><td>0.31675</td><td>7.016616</td><td>0.574790</td><td>117</td><td>156</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 3 × 26\n",
       "\\begin{tabular}{lllllllllllllllllllll}\n",
       " vlogId & mean.pitch & sd.pitch & mean.conf.pitch & sd.conf.pitch & mean.spec.entropy & sd.spec.entropy & mean.val.apeak & sd.val.apeak & mean.loc.apeak & ⋯ & sd.d.energy & avg.voiced.seg & avg.len.seg & time.speaking & voice.rate & num.turns & hogv.entropy & hogv.median & hogv.cogR & hogv.cogC\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & 178.15 & 0.38358 & 1.2526 & 0.45440 & 3.3674 & 0.29309 & 0.82192 & 0.12429 & 0.018525 & ⋯ & 0.0255970 & 0.18441 & 1.3559 & 0.60796 & 0.051389 & 0.44839 & 7.026606 & 0.147870 & 121 & 198\\\\\n",
       "\t VLOG3 & 239.32 & 0.36474 & 1.2205 & 0.41543 & 3.8150 & 0.17479 & 0.64969 & 0.22731 & 0.027022 & ⋯ & 0.0012289 & 0.16404 & 1.0272 & 0.51374 & 0.057632 & 0.50013 & 4.006787 & 0.008571 & 175 & 164\\\\\n",
       "\t VLOG5 & 173.50 & 0.47636 & 1.1678 & 0.50508 & 3.6949 & 0.32347 & 0.65878 & 0.22253 & 0.021466 & ⋯ & 0.0026112 & 0.30966 & 2.2164 & 0.70205 & 0.037614 & 0.31675 & 7.016616 & 0.574790 & 117 & 156\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 3 × 26\n",
       "\n",
       "| vlogId &lt;chr&gt; | mean.pitch &lt;dbl&gt; | sd.pitch &lt;dbl&gt; | mean.conf.pitch &lt;dbl&gt; | sd.conf.pitch &lt;dbl&gt; | mean.spec.entropy &lt;dbl&gt; | sd.spec.entropy &lt;dbl&gt; | mean.val.apeak &lt;dbl&gt; | sd.val.apeak &lt;dbl&gt; | mean.loc.apeak &lt;dbl&gt; | ⋯ ⋯ | sd.d.energy &lt;dbl&gt; | avg.voiced.seg &lt;dbl&gt; | avg.len.seg &lt;dbl&gt; | time.speaking &lt;dbl&gt; | voice.rate &lt;dbl&gt; | num.turns &lt;dbl&gt; | hogv.entropy &lt;dbl&gt; | hogv.median &lt;dbl&gt; | hogv.cogR &lt;dbl&gt; | hogv.cogC &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| VLOG1 | 178.15 | 0.38358 | 1.2526 | 0.45440 | 3.3674 | 0.29309 | 0.82192 | 0.12429 | 0.018525 | ⋯ | 0.0255970 | 0.18441 | 1.3559 | 0.60796 | 0.051389 | 0.44839 | 7.026606 | 0.147870 | 121 | 198 |\n",
       "| VLOG3 | 239.32 | 0.36474 | 1.2205 | 0.41543 | 3.8150 | 0.17479 | 0.64969 | 0.22731 | 0.027022 | ⋯ | 0.0012289 | 0.16404 | 1.0272 | 0.51374 | 0.057632 | 0.50013 | 4.006787 | 0.008571 | 175 | 164 |\n",
       "| VLOG5 | 173.50 | 0.47636 | 1.1678 | 0.50508 | 3.6949 | 0.32347 | 0.65878 | 0.22253 | 0.021466 | ⋯ | 0.0026112 | 0.30966 | 2.2164 | 0.70205 | 0.037614 | 0.31675 | 7.016616 | 0.574790 | 117 | 156 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId mean.pitch sd.pitch mean.conf.pitch sd.conf.pitch mean.spec.entropy\n",
       "1 VLOG1  178.15     0.38358  1.2526          0.45440       3.3674           \n",
       "2 VLOG3  239.32     0.36474  1.2205          0.41543       3.8150           \n",
       "3 VLOG5  173.50     0.47636  1.1678          0.50508       3.6949           \n",
       "  sd.spec.entropy mean.val.apeak sd.val.apeak mean.loc.apeak ⋯ sd.d.energy\n",
       "1 0.29309         0.82192        0.12429      0.018525       ⋯ 0.0255970  \n",
       "2 0.17479         0.64969        0.22731      0.027022       ⋯ 0.0012289  \n",
       "3 0.32347         0.65878        0.22253      0.021466       ⋯ 0.0026112  \n",
       "  avg.voiced.seg avg.len.seg time.speaking voice.rate num.turns hogv.entropy\n",
       "1 0.18441        1.3559      0.60796       0.051389   0.44839   7.026606    \n",
       "2 0.16404        1.0272      0.51374       0.057632   0.50013   4.006787    \n",
       "3 0.30966        2.2164      0.70205       0.037614   0.31675   7.016616    \n",
       "  hogv.median hogv.cogR hogv.cogC\n",
       "1 0.147870    121       198      \n",
       "2 0.008571    175       164      \n",
       "3 0.574790    117       156      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import audiovisual features df\n",
    "audiovisual_features_df = read_delim(AudioVisual_file, delim = \" \") \n",
    "head(audiovisual_features_df, n = 3) # Check the data to see if it is okay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e3b02",
   "metadata": {
    "_cell_guid": "b3b16406-5f3b-45c5-b1a9-9d2f98d66210",
    "_uuid": "cda7cf6b-3680-4451-b1c5-6e276d2d5430",
    "papermill": {
     "duration": 0.020866,
     "end_time": "2023-09-18T11:35:30.943602",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.922736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Feature extraction from transcript texts\n",
    "\n",
    "Here you will develop the code that extract features from the transcript texts using `tidytext`. Look at [Introducing Text Analytics](https://www.kaggle.com/code/datasniffer/introducing-text-analytics-big-5-from-text) to see how you should do this. You may also want to copy and paste the `get_lexicon()` function from there to more easily load various lexicons.\n",
    "\n",
    "It's required to use `tidytext` methods to split and count lexicon keywords as demonstrated in that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f9b479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:30.988502Z",
     "iopub.status.busy": "2023-09-18T11:35:30.986705Z",
     "iopub.status.idle": "2023-09-18T11:35:31.002657Z",
     "shell.execute_reply": "2023-09-18T11:35:31.000749Z"
    },
    "papermill": {
     "duration": 0.041511,
     "end_time": "2023-09-18T11:35:31.005548",
     "exception": false,
     "start_time": "2023-09-18T11:35:30.964037",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to retrieve different lexicons from the 'textdata' package\n",
    "get_lexicon = function(lexicon_name = names(textdata:::download_functions)) {\n",
    "    lexicon_name = match.arg(lexicon_name)\n",
    "    textdata:::download_functions[[lexicon_name]]('.')\n",
    "    rds_filename = paste0(lexicon_name,'.rds')\n",
    "    textdata:::process_functions[[lexicon_name]]('.',rds_filename)\n",
    "    readr::read_rds(rds_filename)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49ef36",
   "metadata": {
    "papermill": {
     "duration": 0.02047,
     "end_time": "2023-09-18T11:35:31.047193",
     "exception": false,
     "start_time": "2023-09-18T11:35:31.026723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Feature extraction from sentiment lexicons\n",
    "\n",
    "Lexicons help in understanding the opinion or emotion in the text. In our analysis, we will incorporate two lexicons: Afinn and NRC.\n",
    "\n",
    "Each of these lexicons are based on single words. These lexicons contain English words. These words are assigned scores that capture positive/negative sentiment, also emotions like suprise, joy, anger, sadness, or surprise.\n",
    "\n",
    "Source: https://afit-r.github.io/sentiment_analysis\n",
    "\n",
    "\n",
    "### 2.1.1 NRC lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537e277",
   "metadata": {
    "papermill": {
     "duration": 0.020636,
     "end_time": "2023-09-18T11:35:31.088594",
     "exception": false,
     "start_time": "2023-09-18T11:35:31.067958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Compute the transcript_features_df dataframe based on the NRC lexicon. The nrc lexicon categorizes words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2391e73f",
   "metadata": {
    "_cell_guid": "f80d89e3-c233-4a5d-ba0c-7f1ca9fb1b04",
    "_uuid": "1540d512-f479-4475-aaa9-45c3acae5178",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:31.133702Z",
     "iopub.status.busy": "2023-09-18T11:35:31.131868Z",
     "iopub.status.idle": "2023-09-18T11:35:36.176749Z",
     "shell.execute_reply": "2023-09-18T11:35:36.174553Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.070446,
     "end_time": "2023-09-18T11:35:36.179635",
     "exception": false,
     "start_time": "2023-09-18T11:35:31.109189",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>filename</th><th scope=col>token</th><th scope=col>sentiment</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>insult</td><td>anger   </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>insult</td><td>disgust </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>insult</td><td>negative</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>insult</td><td>sadness </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>insult</td><td>surprise</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>trade </td><td>trust   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 4\n",
       "\\begin{tabular}{llll}\n",
       " vlogId & filename & token & sentiment\\\\\n",
       " <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & insult & anger   \\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & insult & disgust \\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & insult & negative\\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & insult & sadness \\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & insult & surprise\\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & trade  & trust   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 4\n",
       "\n",
       "| vlogId &lt;chr&gt; | filename &lt;chr&gt; | token &lt;chr&gt; | sentiment &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | insult | anger    |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | insult | disgust  |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | insult | negative |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | insult | sadness  |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | insult | surprise |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | trade  | trust    |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId\n",
       "1 VLOG1 \n",
       "2 VLOG1 \n",
       "3 VLOG1 \n",
       "4 VLOG1 \n",
       "5 VLOG1 \n",
       "6 VLOG1 \n",
       "  filename                                                                         \n",
       "1 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "2 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "3 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "4 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "5 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "6 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "  token  sentiment\n",
       "1 insult anger    \n",
       "2 insult disgust  \n",
       "3 insult negative \n",
       "4 insult sadness  \n",
       "5 insult surprise \n",
       "6 trade  trust    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'vlogId'</li><li>'filename'</li><li>'token'</li><li>'sentiment'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'vlogId'\n",
       "\\item 'filename'\n",
       "\\item 'token'\n",
       "\\item 'sentiment'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'vlogId'\n",
       "2. 'filename'\n",
       "3. 'token'\n",
       "4. 'sentiment'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"vlogId\"    \"filename\"  \"token\"     \"sentiment\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>sentiment</th><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>anger       </td><td> 8</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>anticipation</td><td>10</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>disgust     </td><td> 8</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>fear        </td><td> 6</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>joy         </td><td> 7</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>negative    </td><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " vlogId & sentiment & n\\\\\n",
       " <chr> & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & anger        &  8\\\\\n",
       "\t VLOG1 & anticipation & 10\\\\\n",
       "\t VLOG1 & disgust      &  8\\\\\n",
       "\t VLOG1 & fear         &  6\\\\\n",
       "\t VLOG1 & joy          &  7\\\\\n",
       "\t VLOG1 & negative     & 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 3\n",
       "\n",
       "| vlogId &lt;chr&gt; | sentiment &lt;chr&gt; | n &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| VLOG1 | anger        |  8 |\n",
       "| VLOG1 | anticipation | 10 |\n",
       "| VLOG1 | disgust      |  8 |\n",
       "| VLOG1 | fear         |  6 |\n",
       "| VLOG1 | joy          |  7 |\n",
       "| VLOG1 | negative     | 10 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId sentiment    n \n",
       "1 VLOG1  anger         8\n",
       "2 VLOG1  anticipation 10\n",
       "3 VLOG1  disgust       8\n",
       "4 VLOG1  fear          6\n",
       "5 VLOG1  joy           7\n",
       "6 VLOG1  negative     10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize transcripts by word\n",
    "transcripts_tokenized = \n",
    "    transcripts_df %>%\n",
    "    unnest_tokens(token, TEXT, token = 'words')\n",
    "\n",
    "# Import & remove stopwords\n",
    "stopwords = get_stopwords()\n",
    "transcripts_tokenized = \n",
    "    transcripts_tokenized %>%\n",
    "    anti_join(stopwords, by = c(token = \"word\")) \n",
    "\n",
    "# Import \"nrc\" lexicon\n",
    "nrc = get_lexicon('nrc')\n",
    "\n",
    "# Label the sentiments of each word\n",
    "transcripts_tokenized = \n",
    "    transcripts_tokenized %>%\n",
    "    left_join(nrc,\n",
    "              by = c(token = 'word'),\n",
    "              relationship = 'many-to-many')\n",
    "\n",
    "# Look at results\n",
    "transcripts_tokenized %>%\n",
    "    filter(!is.na(sentiment)) %>% \n",
    "    head()\n",
    "names(transcripts_tokenized)\n",
    "\n",
    "# Compute the sentiment scores and save it in transcript_features_df\n",
    "transcript_features_df = \n",
    "    transcripts_tokenized %>%\n",
    "    count(vlogId, sentiment) \n",
    "\n",
    "# Check results\n",
    "transcript_features_df %>%\n",
    "    head()\n",
    "\n",
    "# Finally, pivot _wider so each vlogger is a column\n",
    "transcript_features_df = \n",
    "    transcript_features_df %>%\n",
    "    pivot_wider(names_from = sentiment,\n",
    "                values_from = n,\n",
    "                values_fill = 0)\n",
    "\n",
    "# And Change the name of the \"NA\" column, which are simply words not associated with any sentiments (i.e., neutral)\n",
    "names(transcript_features_df) = c(names(transcript_features_df)[-length(transcript_features_df)],\n",
    "                                 \"nrc_neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d992d",
   "metadata": {
    "papermill": {
     "duration": 0.021342,
     "end_time": "2023-09-18T11:35:36.222382",
     "exception": false,
     "start_time": "2023-09-18T11:35:36.201040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1.2 AFINN lexicon\n",
    "Compute the transcript_features_df dataframe based on the AFINN lexicon. The AFINN lexicon assigns a score between -5 and 5 to each word. Negative scores indicate negative sentiment and positive scores indicate positive sentiment (https://afit-r.github.io/sentiment_analysis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be388219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:36.270567Z",
     "iopub.status.busy": "2023-09-18T11:35:36.268752Z",
     "iopub.status.idle": "2023-09-18T11:35:38.240855Z",
     "shell.execute_reply": "2023-09-18T11:35:38.239021Z"
    },
    "papermill": {
     "duration": 1.99911,
     "end_time": "2023-09-18T11:35:38.243508",
     "exception": false,
     "start_time": "2023-09-18T11:35:36.244398",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>filename</th><th scope=col>token</th><th scope=col>value</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>like  </td><td> 2</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>insult</td><td>-2</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>cool  </td><td> 1</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>hell  </td><td>-4</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>like  </td><td> 2</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt</td><td>like  </td><td> 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 4\n",
       "\\begin{tabular}{llll}\n",
       " vlogId & filename & token & value\\\\\n",
       " <chr> & <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & like   &  2\\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & insult & -2\\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & cool   &  1\\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & hell   & -4\\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & like   &  2\\\\\n",
       "\t VLOG1 & ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt & like   &  2\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 4\n",
       "\n",
       "| vlogId &lt;chr&gt; | filename &lt;chr&gt; | token &lt;chr&gt; | value &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | like   |  2 |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | insult | -2 |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | cool   |  1 |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | hell   | -4 |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | like   |  2 |\n",
       "| VLOG1 | ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt | like   |  2 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId\n",
       "1 VLOG1 \n",
       "2 VLOG1 \n",
       "3 VLOG1 \n",
       "4 VLOG1 \n",
       "5 VLOG1 \n",
       "6 VLOG1 \n",
       "  filename                                                                         \n",
       "1 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "2 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "3 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "4 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "5 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "6 ../input/bda-2023-profiling-personality/youtube-personality/transcripts/VLOG1.txt\n",
       "  token  value\n",
       "1 like    2   \n",
       "2 insult -2   \n",
       "3 cool    1   \n",
       "4 hell   -4   \n",
       "5 like    2   \n",
       "6 like    2   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 3 × 12</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>afinn_neg_four</th><th scope=col>afinn_neg_three</th><th scope=col>afinn_neg_two</th><th scope=col>afinn_one</th><th scope=col>afinn_two</th><th scope=col>afinn_three</th><th scope=col>afinn_four</th><th scope=col>afinn_neutral</th><th scope=col>afinn_neg_one</th><th scope=col>afinn_neg_five</th><th scope=col>afinn_five</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1  </td><td>1</td><td>3</td><td>5</td><td>4</td><td>13</td><td>4</td><td>1</td><td>180</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>VLOG10 </td><td>1</td><td>1</td><td>7</td><td>6</td><td> 8</td><td>3</td><td>0</td><td>228</td><td>2</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>VLOG100</td><td>0</td><td>1</td><td>1</td><td>9</td><td> 4</td><td>4</td><td>1</td><td>118</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 3 × 12\n",
       "\\begin{tabular}{llllllllllll}\n",
       " vlogId & afinn\\_neg\\_four & afinn\\_neg\\_three & afinn\\_neg\\_two & afinn\\_one & afinn\\_two & afinn\\_three & afinn\\_four & afinn\\_neutral & afinn\\_neg\\_one & afinn\\_neg\\_five & afinn\\_five\\\\\n",
       " <chr> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t VLOG1   & 1 & 3 & 5 & 4 & 13 & 4 & 1 & 180 & 0 & 0 & 0\\\\\n",
       "\t VLOG10  & 1 & 1 & 7 & 6 &  8 & 3 & 0 & 228 & 2 & 0 & 0\\\\\n",
       "\t VLOG100 & 0 & 1 & 1 & 9 &  4 & 4 & 1 & 118 & 1 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 3 × 12\n",
       "\n",
       "| vlogId &lt;chr&gt; | afinn_neg_four &lt;int&gt; | afinn_neg_three &lt;int&gt; | afinn_neg_two &lt;int&gt; | afinn_one &lt;int&gt; | afinn_two &lt;int&gt; | afinn_three &lt;int&gt; | afinn_four &lt;int&gt; | afinn_neutral &lt;int&gt; | afinn_neg_one &lt;int&gt; | afinn_neg_five &lt;int&gt; | afinn_five &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| VLOG1   | 1 | 3 | 5 | 4 | 13 | 4 | 1 | 180 | 0 | 0 | 0 |\n",
       "| VLOG10  | 1 | 1 | 7 | 6 |  8 | 3 | 0 | 228 | 2 | 0 | 0 |\n",
       "| VLOG100 | 0 | 1 | 1 | 9 |  4 | 4 | 1 | 118 | 1 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  afinn_neg_four afinn_neg_three afinn_neg_two afinn_one afinn_two\n",
       "1 VLOG1   1              3               5             4         13       \n",
       "2 VLOG10  1              1               7             6          8       \n",
       "3 VLOG100 0              1               1             9          4       \n",
       "  afinn_three afinn_four afinn_neutral afinn_neg_one afinn_neg_five afinn_five\n",
       "1 4           1          180           0             0              0         \n",
       "2 3           0          228           2             0              0         \n",
       "3 4           1          118           1             0              0         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize transcripts by word\n",
    "transcripts_tokenized = \n",
    "    transcripts_df %>%\n",
    "    unnest_tokens(token, TEXT, token = 'words')\n",
    "\n",
    "# Import & remove stopwords\n",
    "stopwords = get_stopwords()\n",
    "transcripts_tokenized = \n",
    "    transcripts_tokenized %>%\n",
    "    anti_join(stopwords, by = c(token = \"word\"))\n",
    "\n",
    "# Import \"afinn\" lexicon\n",
    "afinn = get_lexicon('afinn')\n",
    "\n",
    "# Do an left join an essay token data fame and afinn word list\n",
    "afinn_token_labeled = left_join(transcripts_tokenized,\n",
    "                                afinn, by = c(token = 'word'),\n",
    "                                relationship = 'many-to-many')\n",
    "\n",
    "# Peek at the result\n",
    "afinn_token_labeled %>%\n",
    "    filter(!is.na(value)) %>%\n",
    "    head()\n",
    "\n",
    "token_afinn_scores = \n",
    "   afinn_token_labeled %>%\n",
    "    count(`vlogId`, value) \n",
    "\n",
    "# Pivot _wider so each vlogger is a column\n",
    "afinn_value = \n",
    "    token_afinn_scores  %>%\n",
    "    pivot_wider(id_cols = 'vlogId',\n",
    "                names_from = value,\n",
    "                values_from = n,\n",
    "                values_fill = 0)\n",
    "\n",
    "# Change the name of the \"NA\" column, which are simply words not associated with any sentiments (i.e., neutral)\n",
    "names(afinn_value) = c('vlogId','afinn_neg_four','afinn_neg_three',\n",
    "                       'afinn_neg_two','afinn_one','afinn_two',\n",
    "                       'afinn_three','afinn_four','afinn_neutral',\n",
    "                       'afinn_neg_one','afinn_neg_five','afinn_five')\n",
    "\n",
    "# Check results\n",
    "afinn_value %>% head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adeb978",
   "metadata": {
    "papermill": {
     "duration": 0.021755,
     "end_time": "2023-09-18T11:35:38.287313",
     "exception": false,
     "start_time": "2023-09-18T11:35:38.265558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Additional text-based features\n",
    "Now we are going to add four additional features: a proportion of long words, an average sentence length, a proportion of long sentences, and a stop word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6efaf13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:38.335698Z",
     "iopub.status.busy": "2023-09-18T11:35:38.333853Z",
     "iopub.status.idle": "2023-09-18T11:35:38.554558Z",
     "shell.execute_reply": "2023-09-18T11:35:38.552535Z"
    },
    "papermill": {
     "duration": 0.247777,
     "end_time": "2023-09-18T11:35:38.557166",
     "exception": false,
     "start_time": "2023-09-18T11:35:38.309389",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 1. PROPORTION OF LONG WORDS (prop_longwords)\n",
    "# (1a) Create a total word count feature\n",
    "word_count <- transcripts_tokenized %>%\n",
    "    group_by(`vlogId`) %>%\n",
    "    count() %>%\n",
    "    ungroup() \n",
    "# Rename n to word_count and take a look to verify\n",
    "word_count <- rename(word_count, word_count = n)\n",
    "\n",
    "# (1b) Create the longwords_count feature \n",
    "# First create a vector of the tokens \n",
    "vector_tokens <- transcripts_tokenized %>%\n",
    "    select(`token`) %>%\n",
    "    unlist() \n",
    "\n",
    "# Create a new vector that gives each token a 1 if > 6 letters and 0 if not\n",
    "longwords_yes_no <- ifelse(nchar(vector_tokens) > 6, 1, 0)\n",
    "\n",
    "# Now we can create a longwords count feature \n",
    "longwords_df <- transcripts_tokenized %>%\n",
    "    mutate(longwords_yes_no = longwords_yes_no) %>%\n",
    "    group_by(`vlogId`) %>%\n",
    "    summarize(longwords_count = sum(longwords_yes_no))\n",
    "\n",
    "# (1c) Сalculate the proportion of long words\n",
    "prop_longwords_df <- word_count %>%\n",
    "    left_join(longwords_df, by = \"vlogId\") %>%\n",
    "    mutate(prop_longwords = longwords_count / word_count)  %>%\n",
    "    select(-c(word_count, longwords_count))\n",
    "\n",
    "# (1d) Add prop_longwords to the transcripts_tokenized df\n",
    "transcript_features_df <- transcript_features_df %>%\n",
    "    left_join(prop_longwords_df, by = \"vlogId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8d1f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:38.644537Z",
     "iopub.status.busy": "2023-09-18T11:35:38.642859Z",
     "iopub.status.idle": "2023-09-18T11:35:38.776315Z",
     "shell.execute_reply": "2023-09-18T11:35:38.773321Z"
    },
    "papermill": {
     "duration": 0.161549,
     "end_time": "2023-09-18T11:35:38.780116",
     "exception": false,
     "start_time": "2023-09-18T11:35:38.618567",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 2. AVERAGE SENTENCE LENGHT (sent_length)\n",
    "#  First we have to create a sentence df\n",
    "sentence_df = \n",
    "    transcripts_df %>%\n",
    "    unnest_tokens(token, TEXT, token = 'sentences')\n",
    "\n",
    "# Create a vector of the sentences \n",
    "vector_sentences <- sentence_df %>%\n",
    "    select('token')  %>%\n",
    "    unlist()\n",
    "\n",
    "# Create a new vector that gives the amount of letters for each sentence\n",
    "sentence_length <- nchar(vector_sentences)\n",
    "\n",
    "# Сalculate the average sentence length\n",
    "sentence_length_df <- sentence_df %>%\n",
    "    mutate(sent_length = sentence_length) %>%\n",
    "    group_by(`vlogId`) %>%\n",
    "    summarize(sent_length = mean(sent_length))\n",
    "\n",
    "# Add sent_length to the transcripts_tokenized df\n",
    "transcript_features_df <- transcript_features_df %>%\n",
    "    left_join(sentence_length_df, by = \"vlogId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63b2527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:38.827997Z",
     "iopub.status.busy": "2023-09-18T11:35:38.826343Z",
     "iopub.status.idle": "2023-09-18T11:35:38.913493Z",
     "shell.execute_reply": "2023-09-18T11:35:38.911550Z"
    },
    "papermill": {
     "duration": 0.114146,
     "end_time": "2023-09-18T11:35:38.915975",
     "exception": false,
     "start_time": "2023-09-18T11:35:38.801829",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 3. PROPORTION OF LONG SENTENCES (prop_longsent)\n",
    "# Calculate a total sentence count \n",
    "sentence_count <- sentence_df %>%\n",
    "    group_by(`vlogId`) %>%\n",
    "    count() %>%\n",
    "    ungroup()\n",
    "\n",
    "# Rename n to sentence_count \n",
    "sentence_count <- rename(sentence_count, sentence_count = n)\n",
    "\n",
    "# Add a long sentences (> 167 letters, mean + 1 sd) feature to the df\n",
    "longsentence_yes_no <- ifelse(nchar(vector_sentences) > 167, 1, 0)\n",
    "\n",
    "# Calculate a long sentences count\n",
    "longsentence_df <- sentence_df %>%\n",
    "    mutate(longsentence_yes_no = longsentence_yes_no) %>%\n",
    "    group_by(`vlogId`) %>%\n",
    "    summarize(longsent_count = sum(longsentence_yes_no))\n",
    "\n",
    "# Calculate a proportion of long sentences \n",
    "sentence_count <- sentence_count %>%\n",
    "    left_join(longsentence_df, by = \"vlogId\") %>%\n",
    "    mutate(prop_longsent = longsent_count / sentence_count)  %>%\n",
    "    select (-c(sentence_count, longsent_count))\n",
    "\n",
    "## Add prop_longsent to the transcripts_tokenized df\n",
    "transcript_features_df <- transcript_features_df %>%\n",
    "    left_join(sentence_count, by = \"vlogId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "575c9167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:38.963193Z",
     "iopub.status.busy": "2023-09-18T11:35:38.961541Z",
     "iopub.status.idle": "2023-09-18T11:35:39.016493Z",
     "shell.execute_reply": "2023-09-18T11:35:39.014528Z"
    },
    "papermill": {
     "duration": 0.081384,
     "end_time": "2023-09-18T11:35:39.019090",
     "exception": false,
     "start_time": "2023-09-18T11:35:38.937706",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 4. STOP WORD COUNT (stopw_count)\n",
    "# First we get the stop words\n",
    "stopwords1 <- get_stopwords()\n",
    "# Let's also add some stopwords we found by looking at the vlog texts and are not included in get_stopwords()\n",
    "stopwords2 <- tibble(word = c(\"just\", \"pretty\", \"uh\", \"one\", \"like\", \"xxxx\",\n",
    "                              \"really\", \"oh\", \"yeah\", \"um\", \"okay\"))\n",
    "# Put them all in one tibble\n",
    "stopwords <- bind_rows(stopwords1, stopwords2)\n",
    "\n",
    "# Count the stopwords\n",
    "stopwords_df <- transcripts_tokenized %>%\n",
    "    semi_join(stopwords, by = c(token = \"word\")) %>%\n",
    "    group_by(`vlogId`) %>%\n",
    "    summarize(stopw_count = n())\n",
    "\n",
    "# Add to the transcripts_tokenized df\n",
    "transcript_features_df <- transcript_features_df %>%\n",
    "    left_join(stopwords_df, by = \"vlogId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb19bc27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:39.066661Z",
     "iopub.status.busy": "2023-09-18T11:35:39.064957Z",
     "iopub.status.idle": "2023-09-18T11:35:39.111154Z",
     "shell.execute_reply": "2023-09-18T11:35:39.109337Z"
    },
    "papermill": {
     "duration": 0.07287,
     "end_time": "2023-09-18T11:35:39.113782",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.040912",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 3 × 16</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>anger</th><th scope=col>anticipation</th><th scope=col>disgust</th><th scope=col>fear</th><th scope=col>joy</th><th scope=col>negative</th><th scope=col>positive</th><th scope=col>sadness</th><th scope=col>surprise</th><th scope=col>trust</th><th scope=col>nrc_neutral</th><th scope=col>prop_longwords</th><th scope=col>sent_length</th><th scope=col>prop_longsent</th><th scope=col>stopw_count</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1  </td><td> 8</td><td>10</td><td>8</td><td> 6</td><td>7</td><td>10</td><td>14</td><td>8</td><td> 5</td><td>11</td><td>182</td><td>0.1800948</td><td> 58.28947</td><td>0.02631579</td><td>21</td></tr>\n",
       "\t<tr><td>VLOG10 </td><td>10</td><td>19</td><td>7</td><td>14</td><td>9</td><td>18</td><td>19</td><td>8</td><td>10</td><td>17</td><td>206</td><td>0.3554688</td><td>120.80952</td><td>0.23809524</td><td> 6</td></tr>\n",
       "\t<tr><td>VLOG100</td><td> 0</td><td> 9</td><td>0</td><td> 2</td><td>8</td><td> 3</td><td>11</td><td>1</td><td> 2</td><td> 9</td><td>121</td><td>0.1582734</td><td>123.08333</td><td>0.25000000</td><td>36</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 3 × 16\n",
       "\\begin{tabular}{llllllllllllllll}\n",
       " vlogId & anger & anticipation & disgust & fear & joy & negative & positive & sadness & surprise & trust & nrc\\_neutral & prop\\_longwords & sent\\_length & prop\\_longsent & stopw\\_count\\\\\n",
       " <chr> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <dbl> & <dbl> & <dbl> & <int>\\\\\n",
       "\\hline\n",
       "\t VLOG1   &  8 & 10 & 8 &  6 & 7 & 10 & 14 & 8 &  5 & 11 & 182 & 0.1800948 &  58.28947 & 0.02631579 & 21\\\\\n",
       "\t VLOG10  & 10 & 19 & 7 & 14 & 9 & 18 & 19 & 8 & 10 & 17 & 206 & 0.3554688 & 120.80952 & 0.23809524 &  6\\\\\n",
       "\t VLOG100 &  0 &  9 & 0 &  2 & 8 &  3 & 11 & 1 &  2 &  9 & 121 & 0.1582734 & 123.08333 & 0.25000000 & 36\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 3 × 16\n",
       "\n",
       "| vlogId &lt;chr&gt; | anger &lt;int&gt; | anticipation &lt;int&gt; | disgust &lt;int&gt; | fear &lt;int&gt; | joy &lt;int&gt; | negative &lt;int&gt; | positive &lt;int&gt; | sadness &lt;int&gt; | surprise &lt;int&gt; | trust &lt;int&gt; | nrc_neutral &lt;int&gt; | prop_longwords &lt;dbl&gt; | sent_length &lt;dbl&gt; | prop_longsent &lt;dbl&gt; | stopw_count &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| VLOG1   |  8 | 10 | 8 |  6 | 7 | 10 | 14 | 8 |  5 | 11 | 182 | 0.1800948 |  58.28947 | 0.02631579 | 21 |\n",
       "| VLOG10  | 10 | 19 | 7 | 14 | 9 | 18 | 19 | 8 | 10 | 17 | 206 | 0.3554688 | 120.80952 | 0.23809524 |  6 |\n",
       "| VLOG100 |  0 |  9 | 0 |  2 | 8 |  3 | 11 | 1 |  2 |  9 | 121 | 0.1582734 | 123.08333 | 0.25000000 | 36 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  anger anticipation disgust fear joy negative positive sadness\n",
       "1 VLOG1    8    10           8        6   7   10       14       8      \n",
       "2 VLOG10  10    19           7       14   9   18       19       8      \n",
       "3 VLOG100  0     9           0        2   8    3       11       1      \n",
       "  surprise trust nrc_neutral prop_longwords sent_length prop_longsent\n",
       "1  5       11    182         0.1800948       58.28947   0.02631579   \n",
       "2 10       17    206         0.3554688      120.80952   0.23809524   \n",
       "3  2        9    121         0.1582734      123.08333   0.25000000   \n",
       "  stopw_count\n",
       "1 21         \n",
       "2  6         \n",
       "3 36         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the final set of transcript features for the first vloggers in `transcript_features_df`\n",
    "transcript_features_df %>% head(n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819cac7",
   "metadata": {
    "_cell_guid": "db396ed9-f94c-48f9-b2ac-1be5792d2a89",
    "_uuid": "f8ba05cf-8815-4bb2-b49b-9eccdd6c2762",
    "papermill": {
     "duration": 0.022138,
     "end_time": "2023-09-18T11:35:39.158180",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.136042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Merging all features\n",
    "After computing the features from the transcript texts and storing it in a data frame, we merge it with the `vlogger_df` dataframe and the `audiovisual_features_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "075e9eba",
   "metadata": {
    "_cell_guid": "f8532d3c-a6d0-430b-82bc-496bc93587e6",
    "_uuid": "471f379b-78cb-40bc-a81a-4e74969ec4d6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:39.206415Z",
     "iopub.status.busy": "2023-09-18T11:35:39.204767Z",
     "iopub.status.idle": "2023-09-18T11:35:39.366458Z",
     "shell.execute_reply": "2023-09-18T11:35:39.363826Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.189367,
     "end_time": "2023-09-18T11:35:39.369786",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.180419",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m404\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m26\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \" \"\n",
      "\u001b[31mchr\u001b[39m  (1): vlogId\n",
      "\u001b[32mdbl\u001b[39m (25): mean.pitch, sd.pitch, mean.conf.pitch, sd.conf.pitch, mean.spec.en...\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 58</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>gender</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th><th scope=col>anger</th><th scope=col>anticipation</th><th scope=col>disgust</th><th scope=col>⋯</th><th scope=col>sd.d.energy</th><th scope=col>avg.voiced.seg</th><th scope=col>avg.len.seg</th><th scope=col>time.speaking</th><th scope=col>voice.rate</th><th scope=col>num.turns</th><th scope=col>hogv.entropy</th><th scope=col>hogv.median</th><th scope=col>hogv.cogR</th><th scope=col>hogv.cogC</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG3</td><td>Female</td><td>5.0</td><td>5.0</td><td>4.6</td><td>5.3</td><td>4.4</td><td> 1</td><td>11</td><td> 2</td><td>⋯</td><td>0.0012289</td><td>0.16404</td><td>1.02720</td><td>0.51374</td><td>0.057632</td><td>0.50013</td><td>4.006787</td><td>0.008571</td><td>175</td><td>164</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG5</td><td>Male  </td><td>5.9</td><td>5.3</td><td>5.3</td><td>5.8</td><td>5.5</td><td> 1</td><td> 6</td><td> 1</td><td>⋯</td><td>0.0026112</td><td>0.30966</td><td>2.21640</td><td>0.70205</td><td>0.037614</td><td>0.31675</td><td>7.016616</td><td>0.574790</td><td>117</td><td>156</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG6</td><td>Male  </td><td>5.4</td><td>4.8</td><td>4.4</td><td>4.8</td><td>5.7</td><td> 5</td><td>11</td><td> 4</td><td>⋯</td><td>0.0148060</td><td>0.19399</td><td>2.53510</td><td>0.75993</td><td>0.048036</td><td>0.29976</td><td>3.465855</td><td>0.008744</td><td>108</td><td>179</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG7</td><td>Male  </td><td>4.7</td><td>5.1</td><td>4.4</td><td>5.1</td><td>4.7</td><td>12</td><td>22</td><td>11</td><td>⋯</td><td>0.0432300</td><td>0.56000</td><td>1.72040</td><td>0.60069</td><td>0.024801</td><td>0.34916</td><td>7.160260</td><td>0.285714</td><td>135</td><td>156</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG8</td><td>Female</td><td> NA</td><td> NA</td><td> NA</td><td> NA</td><td> NA</td><td> 3</td><td>12</td><td> 2</td><td>⋯</td><td>0.0158740</td><td>0.16954</td><td>0.84412</td><td>0.46439</td><td>0.056864</td><td>0.55015</td><td>7.612877</td><td>0.418219</td><td>123</td><td>178</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG9</td><td>Female</td><td>5.6</td><td>5.0</td><td>4.0</td><td>4.2</td><td>4.9</td><td> 9</td><td>17</td><td> 8</td><td>⋯</td><td>0.0006667</td><td>0.18044</td><td>1.61860</td><td>0.67458</td><td>0.054172</td><td>0.41678</td><td>7.032778</td><td>0.120711</td><td>110</td><td>156</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 58\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & vlogId & gender & Extr & Agr & Cons & Emot & Open & anger & anticipation & disgust & ⋯ & sd.d.energy & avg.voiced.seg & avg.len.seg & time.speaking & voice.rate & num.turns & hogv.entropy & hogv.median & hogv.cogR & hogv.cogC\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <int> & <int> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG3 & Female & 5.0 & 5.0 & 4.6 & 5.3 & 4.4 &  1 & 11 &  2 & ⋯ & 0.0012289 & 0.16404 & 1.02720 & 0.51374 & 0.057632 & 0.50013 & 4.006787 & 0.008571 & 175 & 164\\\\\n",
       "\t2 & VLOG5 & Male   & 5.9 & 5.3 & 5.3 & 5.8 & 5.5 &  1 &  6 &  1 & ⋯ & 0.0026112 & 0.30966 & 2.21640 & 0.70205 & 0.037614 & 0.31675 & 7.016616 & 0.574790 & 117 & 156\\\\\n",
       "\t3 & VLOG6 & Male   & 5.4 & 4.8 & 4.4 & 4.8 & 5.7 &  5 & 11 &  4 & ⋯ & 0.0148060 & 0.19399 & 2.53510 & 0.75993 & 0.048036 & 0.29976 & 3.465855 & 0.008744 & 108 & 179\\\\\n",
       "\t4 & VLOG7 & Male   & 4.7 & 5.1 & 4.4 & 5.1 & 4.7 & 12 & 22 & 11 & ⋯ & 0.0432300 & 0.56000 & 1.72040 & 0.60069 & 0.024801 & 0.34916 & 7.160260 & 0.285714 & 135 & 156\\\\\n",
       "\t5 & VLOG8 & Female &  NA &  NA &  NA &  NA &  NA &  3 & 12 &  2 & ⋯ & 0.0158740 & 0.16954 & 0.84412 & 0.46439 & 0.056864 & 0.55015 & 7.612877 & 0.418219 & 123 & 178\\\\\n",
       "\t6 & VLOG9 & Female & 5.6 & 5.0 & 4.0 & 4.2 & 4.9 &  9 & 17 &  8 & ⋯ & 0.0006667 & 0.18044 & 1.61860 & 0.67458 & 0.054172 & 0.41678 & 7.032778 & 0.120711 & 110 & 156\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 58\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | gender &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; | anger &lt;int&gt; | anticipation &lt;int&gt; | disgust &lt;int&gt; | ⋯ ⋯ | sd.d.energy &lt;dbl&gt; | avg.voiced.seg &lt;dbl&gt; | avg.len.seg &lt;dbl&gt; | time.speaking &lt;dbl&gt; | voice.rate &lt;dbl&gt; | num.turns &lt;dbl&gt; | hogv.entropy &lt;dbl&gt; | hogv.median &lt;dbl&gt; | hogv.cogR &lt;dbl&gt; | hogv.cogC &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | VLOG3 | Female | 5.0 | 5.0 | 4.6 | 5.3 | 4.4 |  1 | 11 |  2 | ⋯ | 0.0012289 | 0.16404 | 1.02720 | 0.51374 | 0.057632 | 0.50013 | 4.006787 | 0.008571 | 175 | 164 |\n",
       "| 2 | VLOG5 | Male   | 5.9 | 5.3 | 5.3 | 5.8 | 5.5 |  1 |  6 |  1 | ⋯ | 0.0026112 | 0.30966 | 2.21640 | 0.70205 | 0.037614 | 0.31675 | 7.016616 | 0.574790 | 117 | 156 |\n",
       "| 3 | VLOG6 | Male   | 5.4 | 4.8 | 4.4 | 4.8 | 5.7 |  5 | 11 |  4 | ⋯ | 0.0148060 | 0.19399 | 2.53510 | 0.75993 | 0.048036 | 0.29976 | 3.465855 | 0.008744 | 108 | 179 |\n",
       "| 4 | VLOG7 | Male   | 4.7 | 5.1 | 4.4 | 5.1 | 4.7 | 12 | 22 | 11 | ⋯ | 0.0432300 | 0.56000 | 1.72040 | 0.60069 | 0.024801 | 0.34916 | 7.160260 | 0.285714 | 135 | 156 |\n",
       "| 5 | VLOG8 | Female |  NA |  NA |  NA |  NA |  NA |  3 | 12 |  2 | ⋯ | 0.0158740 | 0.16954 | 0.84412 | 0.46439 | 0.056864 | 0.55015 | 7.612877 | 0.418219 | 123 | 178 |\n",
       "| 6 | VLOG9 | Female | 5.6 | 5.0 | 4.0 | 4.2 | 4.9 |  9 | 17 |  8 | ⋯ | 0.0006667 | 0.18044 | 1.61860 | 0.67458 | 0.054172 | 0.41678 | 7.032778 | 0.120711 | 110 | 156 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId gender Extr Agr Cons Emot Open anger anticipation disgust ⋯\n",
       "1 VLOG3  Female 5.0  5.0 4.6  5.3  4.4   1    11            2      ⋯\n",
       "2 VLOG5  Male   5.9  5.3 5.3  5.8  5.5   1     6            1      ⋯\n",
       "3 VLOG6  Male   5.4  4.8 4.4  4.8  5.7   5    11            4      ⋯\n",
       "4 VLOG7  Male   4.7  5.1 4.4  5.1  4.7  12    22           11      ⋯\n",
       "5 VLOG8  Female  NA   NA  NA   NA   NA   3    12            2      ⋯\n",
       "6 VLOG9  Female 5.6  5.0 4.0  4.2  4.9   9    17            8      ⋯\n",
       "  sd.d.energy avg.voiced.seg avg.len.seg time.speaking voice.rate num.turns\n",
       "1 0.0012289   0.16404        1.02720     0.51374       0.057632   0.50013  \n",
       "2 0.0026112   0.30966        2.21640     0.70205       0.037614   0.31675  \n",
       "3 0.0148060   0.19399        2.53510     0.75993       0.048036   0.29976  \n",
       "4 0.0432300   0.56000        1.72040     0.60069       0.024801   0.34916  \n",
       "5 0.0158740   0.16954        0.84412     0.46439       0.056864   0.55015  \n",
       "6 0.0006667   0.18044        1.61860     0.67458       0.054172   0.41678  \n",
       "  hogv.entropy hogv.median hogv.cogR hogv.cogC\n",
       "1 4.006787     0.008571    175       164      \n",
       "2 7.016616     0.574790    117       156      \n",
       "3 3.465855     0.008744    108       179      \n",
       "4 7.160260     0.285714    135       156      \n",
       "5 7.612877     0.418219    123       178      \n",
       "6 7.032778     0.120711    110       156      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload \"vlogger_df\" and \"audivisual_features_df\"\n",
    "vlogger_df = left_join(gender_df, pers_df, by = 'vlogId')\n",
    "audiovisual_features_df = read_delim(AudioVisual_file, delim = \" \") \n",
    "\n",
    "# Merge `vlogger_df` with `transcript_features_df` into `vlogger_df`\n",
    "vlogger_df =\n",
    "    vlogger_df %>%\n",
    "    inner_join(transcript_features_df, by = \"vlogId\")\n",
    "\n",
    "# Merge `vlogger_df` with `affin_value` into `vlogger_df`\n",
    "vlogger_df =\n",
    "    left_join(vlogger_df, afinn_value, by = \"vlogId\")\n",
    "\n",
    "# Merge vlogger_df` with `audiovisual_features_df` into `vlogger_df\n",
    "vlogger_df =\n",
    "    vlogger_df %>%\n",
    "    inner_join(audiovisual_features_df, by = \"vlogId\")\n",
    "\n",
    "# Look at result\n",
    "head(vlogger_df, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ca1f3",
   "metadata": {
    "_cell_guid": "ba49ed13-9045-4b21-9f2e-09b58bc02b41",
    "_uuid": "54aafd4b-9834-4086-8030-702069f39518",
    "papermill": {
     "duration": 0.023106,
     "end_time": "2023-09-18T11:35:39.415817",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.392711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Predictive model\n",
    "\n",
    "## 3.1 Compare linear models with and without polinomial terms\n",
    "\n",
    "First, we will fit a baseline Linear Model with all predictions and no polinomial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3558f9c2",
   "metadata": {
    "_cell_guid": "8a713b08-df61-4c46-b392-1084c9f357b4",
    "_uuid": "972d1589-1ba3-49b3-bae8-9e09c7666a21",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:39.465715Z",
     "iopub.status.busy": "2023-09-18T11:35:39.463916Z",
     "iopub.status.idle": "2023-09-18T11:35:39.548464Z",
     "shell.execute_reply": "2023-09-18T11:35:39.546535Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.112214,
     "end_time": "2023-09-18T11:35:39.550963",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.438749",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.659256222718298"
      ],
      "text/latex": [
       "0.659256222718298"
      ],
      "text/markdown": [
       "0.659256222718298"
      ],
      "text/plain": [
       "[1] 0.6592562"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "53"
      ],
      "text/latex": [
       "53"
      ],
      "text/markdown": [
       "53"
      ],
      "text/plain": [
       "[1] 53"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit full model with all the predictors\n",
    "fit_mlm = lm(cbind(Extr, Agr, Cons, Emot, Open) ~ ., data = vlogger_df[,-1])\n",
    "\n",
    "# RMSE (training data)\n",
    "sqrt(mean(fit_mlm$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "length(fit_mlm$coef[,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099362e7",
   "metadata": {
    "papermill": {
     "duration": 0.023118,
     "end_time": "2023-09-18T11:35:39.597351",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.574233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, let's add the second-order polinomial terms for all quantitative variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b591b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:39.647813Z",
     "iopub.status.busy": "2023-09-18T11:35:39.645816Z",
     "iopub.status.idle": "2023-09-18T11:35:39.694493Z",
     "shell.execute_reply": "2023-09-18T11:35:39.692588Z"
    },
    "papermill": {
     "duration": 0.076349,
     "end_time": "2023-09-18T11:35:39.696979",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.620630",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.56102095719246"
      ],
      "text/latex": [
       "0.56102095719246"
      ],
      "text/markdown": [
       "0.56102095719246"
      ],
      "text/plain": [
       "[1] 0.561021"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "104"
      ],
      "text/latex": [
       "104"
      ],
      "text/markdown": [
       "104"
      ],
      "text/plain": [
       "[1] 104"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a model with a second-order polinomial term for each of the predictors except gender (full model)\n",
    "fit_mlm2 = lm(cbind(Extr, Agr, Cons, Emot, Open) ~ . + I(anger^2) + I(anticipation^2) +\n",
    "                I(disgust^2) + I(fear^2) + I(joy^2) +\n",
    "                I(negative^2) + I(positive^2) + I(sadness^2) +\n",
    "                I(surprise^2) + I(trust^2) + I(nrc_neutral^2) +\n",
    "                I(prop_longwords^2) + I(sent_length^2) + I(prop_longsent^2) + I(stopw_count^2) +\n",
    "                I(afinn_neg_four^2) + I(afinn_neg_three^2) + I(afinn_neg_two^2) +\n",
    "                I(afinn_one^2) + I(afinn_two^2) + I(afinn_three^2) +\n",
    "                I(afinn_four^2) + I(afinn_neutral^2) + I(afinn_neg_one^2) +\n",
    "                I(afinn_neg_five^2) + I(afinn_five^2) + I(mean.pitch^2) +      \n",
    "                I(sd.pitch^2) + I(mean.conf.pitch^2) + I(sd.conf.pitch^2) +    \n",
    "                I(mean.spec.entropy^2) + I(sd.spec.entropy^2) + I(mean.val.apeak^2) +   \n",
    "                I(sd.val.apeak^2) + I(mean.loc.apeak^2) + I(sd.loc.apeak^2) +     \n",
    "                I(mean.num.apeak^2) + I(sd.num.apeak^2) + I(mean.energy^2) +      \n",
    "                I(sd.energy^2) + I(mean.d.energy^2) + I(sd.d.energy^2) +      \n",
    "                I(avg.voiced.seg^2) + I(avg.len.seg^2) + I(time.speaking^2) +    \n",
    "                I(voice.rate^2) + I(num.turns^2) + I(hogv.entropy^2) +     \n",
    "                I(hogv.median^2) + I(hogv.cogR^2) + I(hogv.cogC^2),\n",
    "              data = vlogger_df[,-1])\n",
    "\n",
    "# RMSE (training data)\n",
    "sqrt(mean(fit_mlm2$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "length(fit_mlm2$coef[,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d98d1c",
   "metadata": {
    "papermill": {
     "duration": 0.023543,
     "end_time": "2023-09-18T11:35:39.743971",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.720428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we can compare both models with the `anova()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a56e952c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:39.796732Z",
     "iopub.status.busy": "2023-09-18T11:35:39.794974Z",
     "iopub.status.idle": "2023-09-18T11:35:39.881364Z",
     "shell.execute_reply": "2023-09-18T11:35:39.876993Z"
    },
    "papermill": {
     "duration": 0.119588,
     "end_time": "2023-09-18T11:35:39.886886",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.767298",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A anova: 2 × 8</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Res.Df</th><th scope=col>Df</th><th scope=col>Gen.var.</th><th scope=col>Pillai</th><th scope=col>approx F</th><th scope=col>num Df</th><th scope=col>den Df</th><th scope=col>Pr(&gt;F)</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>220</td><td>NA</td><td>0.3806552</td><td>      NA</td><td>      NA</td><td> NA</td><td>  NA</td><td>          NA</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>270</td><td>50</td><td>0.4085149</td><td>1.172644</td><td>1.348094</td><td>250</td><td>1100</td><td>0.0008815814</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A anova: 2 × 8\n",
       "\\begin{tabular}{r|llllllll}\n",
       "  & Res.Df & Df & Gen.var. & Pillai & approx F & num Df & den Df & Pr(>F)\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 220 & NA & 0.3806552 &       NA &       NA &  NA &   NA &           NA\\\\\n",
       "\t2 & 270 & 50 & 0.4085149 & 1.172644 & 1.348094 & 250 & 1100 & 0.0008815814\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A anova: 2 × 8\n",
       "\n",
       "| <!--/--> | Res.Df &lt;dbl&gt; | Df &lt;dbl&gt; | Gen.var. &lt;dbl&gt; | Pillai &lt;dbl&gt; | approx F &lt;dbl&gt; | num Df &lt;dbl&gt; | den Df &lt;dbl&gt; | Pr(&gt;F) &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 220 | NA | 0.3806552 |       NA |       NA |  NA |   NA |           NA |\n",
       "| 2 | 270 | 50 | 0.4085149 | 1.172644 | 1.348094 | 250 | 1100 | 0.0008815814 |\n",
       "\n"
      ],
      "text/plain": [
       "  Res.Df Df Gen.var.  Pillai   approx F num Df den Df Pr(>F)      \n",
       "1 220    NA 0.3806552       NA       NA  NA      NA             NA\n",
       "2 270    50 0.4085149 1.172644 1.348094 250    1100   0.0008815814"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The more complex model seems to be better\n",
    "anova(fit_mlm2, fit_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7d4f8",
   "metadata": {
    "papermill": {
     "duration": 0.027239,
     "end_time": "2023-09-18T11:35:39.963913",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.936674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Due to the large number of coefficients, there is a large risk of overfitting in the model. We will counteract this by taking a stepwise selection for the coefficients. Specifically, we will use mixed stepwise selection to choose the best predictors for our final models. Stepwise selection is a feature selection technique that will iteratively remove the least informative predictors until it arrives at the most relevant subset of features. This method leads to a model for each trait with a unique set of predictors, optimizing model interpretability and preventing overfitting.\n",
    "It may be the case however, that the most important predictors are not the same for each of the personality traits. Hence, we will fit 5 different models, one for each personality trait. For each of these, we will perform stepwise selection to reduce the number of predictors and lower the risk of overfitting. For this task, we will use the `stepAIC()` function from the `MASS` package. Before doing so, however, we will try adding some interaction effects between predictors based on theory and substantive reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4c539",
   "metadata": {
    "papermill": {
     "duration": 0.023861,
     "end_time": "2023-09-18T11:35:40.012382",
     "exception": false,
     "start_time": "2023-09-18T11:35:39.988521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Test adding interaction between predictors\n",
    "\n",
    "We also tried adding different interaction terms to our models, yet none of them led to the considerable decrease in RMSE except for one of the interactions. The code below includes comments detailing the substantive reasoning of why we added these interactions to the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "094d4c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:40.064922Z",
     "iopub.status.busy": "2023-09-18T11:35:40.063192Z",
     "iopub.status.idle": "2023-09-18T11:35:40.346507Z",
     "shell.execute_reply": "2023-09-18T11:35:40.344407Z"
    },
    "papermill": {
     "duration": 0.311965,
     "end_time": "2023-09-18T11:35:40.348933",
     "exception": false,
     "start_time": "2023-09-18T11:35:40.036968",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When predicting Neuroticism, the interaction between negative and sent_lengt decreases the RMSE from 0.6387021 to 0.6385808 \n",
      "When predicting Agreeableness, the interaction between jmean.energy and avg.voiced.seg decreases the RMSE from 0.678881 to 0.6786469 \n",
      "When predicting Extraversion, the interaction between sd.pitch and voice.rate decreases the RMSE from 0.7231238 to 0.7230409 \n",
      "When predicting Agreeableness, the interaction between hogv.cogC and num.turns decreases the RMSE from 0.678881 to 0.6787878 \n",
      "When predicting Conscientiousness, the interaction between hogv.cogC and num.turns decreases the RMSE from 0.6399801 to 0.6392766 \n",
      "When predicting Openness, the interaction between hogv.entropy and hogv.cogR decreases the RMSE from 0.609747 to 0.6095818 \n",
      "When predicting Agreeableness, the interaction between mean.val.apeak and avg.voiced.seg decreases the RMSE from 0.678881 to 0.6786368 \n",
      "When predicting Neuroticism, the interaction between anger and mean.pitch decreases the RMSE from 0.6387021 to 0.6382547 \n",
      "When predicting Extraversion, the interaction between joy and time.speaking decreases the RMSE from 0.7231238 to 0.7175091 \n"
     ]
    }
   ],
   "source": [
    "## Fitting the model with and without interaction term\n",
    "# whether longer sentences expressing negative sentiments are associated with higher neuroticism scores\n",
    "fit_mlm4 <- lm(Emot ~ ., data = vlogger_df[,-c(1,3,4,5,7)])\n",
    "fit_mlm5 <- lm(Emot ~ . + negative * sent_length, data = vlogger_df[,-c(1,3,4,5,7)])\n",
    "\n",
    "# whether individuals with higher energy in their voice and more voiced segments tend to be more agreeable\n",
    "fit_mlm6 <- lm(Agr ~ ., data = vlogger_df[,-c(1,3,5,6,7)])\n",
    "fit_mlm7 <- lm(Agr ~ . + mean.energy * avg.voiced.seg, data = vlogger_df[,-c(1,3,5,6,7)])\n",
    "\n",
    "# whether people with more variable pitch and a faster speaking rate tend to be more extraverted\n",
    "fit_mlm8 <- lm(Extr ~ ., data = vlogger_df[,-c(1,4,5,6,7)])\n",
    "fit_mlm9 <- lm(Extr ~ . + sd.pitch * voice.rate, data = vlogger_df[,-c(1,4,5,6,7)])\n",
    "\n",
    "# whether a more central gaze and a higher number of conversational turns are associated with higher agreeableness scores\n",
    "fit_mlm10 <- lm(Agr ~ ., data = vlogger_df[,-c(1,3,5,6,7)])\n",
    "fit_mlm11 <- lm(Agr ~ . + hogv.cogC * num.turns, data = vlogger_df[,-c(1,3,5,6,7)]) \n",
    "\n",
    "# whether conscientious individuals use longer words when expressing positive sentiments\n",
    "fit_mlm12 <- lm(Cons ~ ., data = vlogger_df[,-c(1,3,4,6,7)])\n",
    "fit_mlm13 <- lm(Cons ~ . + positive * prop_longwords, data = vlogger_df[,-c(1,2,4,6,7)])\n",
    "\n",
    "# whether individuals who exhibit more entropy in their visual behaviors and a central gaze point tend to be more open to experience.\n",
    "fit_mlm14 <- lm(Open ~ ., data = vlogger_df[,-c(1,3,4,5,6)])\n",
    "fit_mlm15 <- lm(Open ~ . + hogv.entropy * hogv.cogR, data = vlogger_df[,-c(1,3,4,5,6)])\n",
    "\n",
    "# whether individuals with a specific vocal peak pattern and more voiced segments tend to be more agreeable\n",
    "fit_mlm16 <- lm(Agr ~ ., data = vlogger_df[,-c(1,3,5,6,7)])\n",
    "fit_mlm17 <- lm(Agr ~ . + mean.val.apeak * avg.voiced.seg, data = vlogger_df[,-c(1,3,5,6,7)]) \n",
    "\n",
    "# how pitch modulation in speech during expressions of anger relates to neuroticism\n",
    "fit_mlm18 <- lm(Emot ~ ., data = vlogger_df[,-c(1,3,4,5,7)])\n",
    "fit_mlm19 <- lm(Emot ~ . + anger * mean.pitch, data = vlogger_df[,-c(1,3,4,5,7)])\n",
    "\n",
    "# whether individuals who express joy while speaking for longer durations tend to be more extraverted\n",
    "fit_mlm20 <- lm(Extr ~ ., data = vlogger_df[,-c(1,4,5,6,7)])\n",
    "fit_mlm21 <- lm(Extr ~ . + joy * time.speaking, data = vlogger_df[,-c(1,4,5,6,7)])\n",
    "\n",
    "## Change in RMSE after adding interactions\n",
    "rmse4 <- sqrt(mean(fit_mlm4$residuals^2))\n",
    "rmse5 <- sqrt(mean(fit_mlm5$residuals^2))\n",
    "cat(\"When predicting Neuroticism, the interaction between negative and sent_lengt decreases the RMSE from\", rmse4, \"to\", rmse5, \"\\n\")\n",
    "\n",
    "rmse6 <- sqrt(mean(fit_mlm6$residuals^2))\n",
    "rmse7 <- sqrt(mean(fit_mlm7$residuals^2))\n",
    "cat(\"When predicting Agreeableness, the interaction between jmean.energy and avg.voiced.seg decreases the RMSE from\", rmse6, \"to\", rmse7, \"\\n\")\n",
    "\n",
    "rmse8 <- sqrt(mean(fit_mlm8$residuals^2))\n",
    "rmse9 <- sqrt(mean(fit_mlm9$residuals^2))\n",
    "cat(\"When predicting Extraversion, the interaction between sd.pitch and voice.rate decreases the RMSE from\", rmse8, \"to\", rmse9, \"\\n\")\n",
    "\n",
    "rmse10 <- sqrt(mean(fit_mlm10$residuals^2))\n",
    "rmse11 <- sqrt(mean(fit_mlm11$residuals^2))\n",
    "cat(\"When predicting Agreeableness, the interaction between hogv.cogC and num.turns decreases the RMSE from\", rmse10, \"to\", rmse11, \"\\n\")\n",
    "\n",
    "rmse12 <- sqrt(mean(fit_mlm12$residuals^2))\n",
    "rmse13 <- sqrt(mean(fit_mlm13$residuals^2))\n",
    "cat(\"When predicting Conscientiousness, the interaction between hogv.cogC and num.turns decreases the RMSE from\", rmse12, \"to\", rmse13, \"\\n\")\n",
    "\n",
    "rmse14 <- sqrt(mean(fit_mlm14$residuals^2))\n",
    "rmse15 <- sqrt(mean(fit_mlm15$residuals^2))\n",
    "cat(\"When predicting Openness, the interaction between hogv.entropy and hogv.cogR decreases the RMSE from\", rmse14, \"to\", rmse15, \"\\n\")\n",
    "\n",
    "rmse16 <- sqrt(mean(fit_mlm16$residuals^2))\n",
    "rmse17 <- sqrt(mean(fit_mlm17$residuals^2))\n",
    "cat(\"When predicting Agreeableness, the interaction between mean.val.apeak and avg.voiced.seg decreases the RMSE from\", rmse16, \"to\", rmse17, \"\\n\")\n",
    "\n",
    "rmse18 <- sqrt(mean(fit_mlm18$residuals^2))\n",
    "rmse19 <- sqrt(mean(fit_mlm19$residuals^2))\n",
    "cat(\"When predicting Neuroticism, the interaction between anger and mean.pitch decreases the RMSE from\", rmse18, \"to\", rmse19, \"\\n\")\n",
    "\n",
    "rmse20 <- sqrt(mean(fit_mlm20$residuals^2))\n",
    "rmse21 <- sqrt(mean(fit_mlm21$residuals^2))\n",
    "cat(\"When predicting Extraversion, the interaction between joy and time.speaking decreases the RMSE from\", rmse20, \"to\", rmse21, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3dc8ef",
   "metadata": {
    "papermill": {
     "duration": 0.024868,
     "end_time": "2023-09-18T11:35:40.398320",
     "exception": false,
     "start_time": "2023-09-18T11:35:40.373452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Based on these results, we will continue modeling with only the inclusion of the interaction between joy and time.speaking for the extraversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14f2e9",
   "metadata": {
    "papermill": {
     "duration": 0.024663,
     "end_time": "2023-09-18T11:35:40.447583",
     "exception": false,
     "start_time": "2023-09-18T11:35:40.422920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 1. Extraversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18f5e683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:40.500337Z",
     "iopub.status.busy": "2023-09-18T11:35:40.498622Z",
     "iopub.status.idle": "2023-09-18T11:35:40.513662Z",
     "shell.execute_reply": "2023-09-18T11:35:40.511740Z"
    },
    "papermill": {
     "duration": 0.044043,
     "end_time": "2023-09-18T11:35:40.516173",
     "exception": false,
     "start_time": "2023-09-18T11:35:40.472130",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create table to summarize stepwise selection results\n",
    "table1 <- data.frame(matrix(NA, 5, 4, dimnames = list(c(\"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Emotional Stability\", \"Openness\"),\n",
    "                                                      c(\"RMSE baseline\", \"Number Predictors Baseline\", \"RMSE reduced\", \"Number Predictors Reduced\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f78fb31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:40.568951Z",
     "iopub.status.busy": "2023-09-18T11:35:40.567233Z",
     "iopub.status.idle": "2023-09-18T11:35:52.084728Z",
     "shell.execute_reply": "2023-09-18T11:35:52.077758Z"
    },
    "papermill": {
     "duration": 11.550102,
     "end_time": "2023-09-18T11:35:52.090779",
     "exception": false,
     "start_time": "2023-09-18T11:35:40.540677",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = formula$call$formula, data = vlogger_df[, -1])\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.77996 -0.47021 -0.00459  0.48084  1.64059 \n",
       "\n",
       "Coefficients:\n",
       "                      Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)          5.433e-01  1.000e+00   0.543 0.587524    \n",
       "genderMale           3.797e-01  1.224e-01   3.102 0.002121 ** \n",
       "anger                2.554e-02  1.501e-02   1.701 0.090061 .  \n",
       "negative            -2.877e-02  1.006e-02  -2.860 0.004559 ** \n",
       "nrc_neutral          4.502e-03  1.171e-03   3.846 0.000148 ***\n",
       "prop_longwords       8.551e+00  3.745e+00   2.284 0.023147 *  \n",
       "sent_length         -2.088e-02  4.938e-03  -4.228 3.20e-05 ***\n",
       "stopw_count         -6.627e-03  3.611e-03  -1.835 0.067550 .  \n",
       "afinn_three          3.273e-02  1.187e-02   2.757 0.006216 ** \n",
       "afinn_four           8.084e-02  2.002e-02   4.039 6.95e-05 ***\n",
       "mean.pitch           2.994e-03  1.084e-03   2.762 0.006134 ** \n",
       "sd.pitch             5.405e+00  1.709e+00   3.163 0.001733 ** \n",
       "sd.spec.entropy     -1.711e+00  7.570e-01  -2.260 0.024564 *  \n",
       "sd.val.apeak        -1.147e+01  5.134e+00  -2.234 0.026286 *  \n",
       "mean.loc.apeak       1.640e+02  3.612e+01   4.541 8.34e-06 ***\n",
       "mean.num.apeak      -3.886e-01  1.112e-01  -3.494 0.000553 ***\n",
       "sd.num.apeak         4.331e-01  1.293e-01   3.349 0.000922 ***\n",
       "mean.energy         -2.742e+01  1.009e+01  -2.718 0.006977 ** \n",
       "sd.d.energy          1.158e+02  2.658e+01   4.358 1.84e-05 ***\n",
       "avg.len.seg         -7.026e-02  2.697e-02  -2.605 0.009683 ** \n",
       "time.speaking        1.575e+00  3.973e-01   3.964 9.38e-05 ***\n",
       "num.turns           -6.085e+00  2.713e+00  -2.243 0.025670 *  \n",
       "hogv.entropy         2.307e-01  6.027e-02   3.829 0.000159 ***\n",
       "hogv.median         -2.805e+00  1.631e+00  -1.720 0.086475 .  \n",
       "hogv.cogR            3.968e-03  2.386e-03   1.663 0.097489 .  \n",
       "I(anticipation^2)   -2.889e-04  1.597e-04  -1.809 0.071486 .  \n",
       "I(trust^2)          -1.962e-04  1.359e-04  -1.444 0.149948    \n",
       "I(nrc_neutral^2)    -1.934e-05  6.080e-06  -3.180 0.001637 ** \n",
       "I(prop_longwords^2) -1.645e+01  7.414e+00  -2.219 0.027323 *  \n",
       "I(sent_length^2)     8.294e-05  2.043e-05   4.059 6.39e-05 ***\n",
       "I(afinn_neg_two^2)   2.320e-03  7.646e-04   3.035 0.002633 ** \n",
       "I(afinn_two^2)       3.129e-04  1.012e-04   3.091 0.002196 ** \n",
       "I(afinn_neutral^2)   1.343e-05  5.608e-06   2.396 0.017247 *  \n",
       "I(sd.pitch^2)       -5.386e+00  1.843e+00  -2.923 0.003751 ** \n",
       "I(sd.val.apeak^2)    2.991e+01  1.219e+01   2.454 0.014740 *  \n",
       "I(mean.loc.apeak^2) -2.093e+03  5.720e+02  -3.658 0.000304 ***\n",
       "I(sd.loc.apeak^2)   -6.878e-01  1.496e-01  -4.598 6.45e-06 ***\n",
       "I(mean.num.apeak^2)  1.980e-02  6.030e-03   3.283 0.001157 ** \n",
       "I(sd.num.apeak^2)   -3.757e-02  1.254e-02  -2.996 0.002984 ** \n",
       "I(mean.energy^2)     1.569e+02  6.414e+01   2.445 0.015081 *  \n",
       "I(sd.d.energy^2)    -2.003e+03  5.510e+02  -3.634 0.000332 ***\n",
       "I(num.turns^2)       6.783e+00  3.959e+00   1.713 0.087773 .  \n",
       "I(hogv.median^2)     5.961e+00  2.902e+00   2.054 0.040905 *  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.6941 on 280 degrees of freedom\n",
       "  (80 observations deleted due to missingness)\n",
       "Multiple R-squared:  0.5524,\tAdjusted R-squared:  0.4853 \n",
       "F-statistic: 8.227 on 42 and 280 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a model with a second-order polinomial term for each of the predictors except gender (full model)\n",
    "fit_extr = lm(Extr ~ . + joy * time.speaking - Agr - Cons - Emot - Open + I(anger^2) + I(anticipation^2) +\n",
    "                I(disgust^2) + I(fear^2) + I(joy^2) +\n",
    "                I(negative^2) + I(positive^2) + I(sadness^2) +\n",
    "                I(surprise^2) + I(trust^2) + I(nrc_neutral^2) +\n",
    "                I(prop_longwords^2) + I(sent_length^2) + I(prop_longsent^2) + I(stopw_count^2) +\n",
    "                I(afinn_neg_four^2) + I(afinn_neg_three^2) + I(afinn_neg_two^2) +\n",
    "                I(afinn_one^2) + I(afinn_two^2) + I(afinn_three^2) +\n",
    "                I(afinn_four^2) + I(afinn_neutral^2) + I(afinn_neg_one^2) +\n",
    "                I(afinn_neg_five^2) + I(afinn_five^2) + I(mean.pitch^2) +      \n",
    "                I(sd.pitch^2) + I(mean.conf.pitch^2) + I(sd.conf.pitch^2) +    \n",
    "                I(mean.spec.entropy^2) + I(sd.spec.entropy^2) + I(mean.val.apeak^2) +   \n",
    "                I(sd.val.apeak^2) + I(mean.loc.apeak^2) + I(sd.loc.apeak^2) +     \n",
    "                I(mean.num.apeak^2) + I(sd.num.apeak^2) + I(mean.energy^2) +      \n",
    "                I(sd.energy^2) + I(mean.d.energy^2) + I(sd.d.energy^2) +      \n",
    "                I(avg.voiced.seg^2) + I(avg.len.seg^2) + I(time.speaking^2) +    \n",
    "                I(voice.rate^2) + I(num.turns^2) + I(hogv.entropy^2) +     \n",
    "                I(hogv.median^2) + I(hogv.cogR^2) + I(hogv.cogC^2),\n",
    "              data = vlogger_df[,-1])\n",
    "\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[1,1] <- sqrt(mean(fit_extr$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[1,2] <- length(fit_extr$coef) \n",
    "\n",
    "# Run Mixed model selection and save formula\n",
    "formula <- stepAIC(fit_extr, direction = \"both\", trace = F)\n",
    "\n",
    "# Recompute regression model with the paramters dropped\n",
    "fit_extr_step = lm(formula$call$formula, data = vlogger_df[,-1])\n",
    "\n",
    "# Summary of final model (after stepwise selection)\n",
    "summary(fit_extr_step)\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[1,3] <- sqrt(mean(fit_extr_step$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[1,4] <- length(fit_extr_step$coef) # 43 (vs 105 in the full model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11580276",
   "metadata": {
    "papermill": {
     "duration": 0.031605,
     "end_time": "2023-09-18T11:35:52.179122",
     "exception": false,
     "start_time": "2023-09-18T11:35:52.147517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 2. Agreeableness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df4358f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:35:52.233483Z",
     "iopub.status.busy": "2023-09-18T11:35:52.231805Z",
     "iopub.status.idle": "2023-09-18T11:35:59.930855Z",
     "shell.execute_reply": "2023-09-18T11:35:59.926543Z"
    },
    "papermill": {
     "duration": 7.732648,
     "end_time": "2023-09-18T11:35:59.936921",
     "exception": false,
     "start_time": "2023-09-18T11:35:52.204273",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = formula$call$formula, data = vlogger_df[, -1])\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.65697 -0.35502  0.03155  0.38267  1.98027 \n",
       "\n",
       "Coefficients:\n",
       "                         Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)             5.936e+00  1.224e+00   4.851 2.12e-06 ***\n",
       "genderMale             -3.895e-01  1.184e-01  -3.290 0.001142 ** \n",
       "anger                  -2.837e-02  1.793e-02  -1.583 0.114698    \n",
       "anticipation            3.428e-02  1.715e-02   1.998 0.046736 *  \n",
       "disgust                -6.544e-02  2.675e-02  -2.446 0.015097 *  \n",
       "fear                    4.060e-02  1.952e-02   2.080 0.038517 *  \n",
       "positive                2.515e-02  8.897e-03   2.827 0.005074 ** \n",
       "surprise                2.498e-02  1.442e-02   1.732 0.084539 .  \n",
       "trust                  -3.801e-02  1.630e-02  -2.331 0.020507 *  \n",
       "prop_longwords          8.689e+00  3.681e+00   2.360 0.019004 *  \n",
       "sent_length             1.403e-02  8.004e-03   1.753 0.080734 .  \n",
       "prop_longsent          -2.283e+00  1.348e+00  -1.694 0.091533 .  \n",
       "afinn_neg_four         -1.474e-01  3.229e-02  -4.565 7.72e-06 ***\n",
       "afinn_neg_three        -1.030e-01  3.504e-02  -2.939 0.003596 ** \n",
       "afinn_neg_two          -4.266e-02  1.659e-02  -2.571 0.010714 *  \n",
       "afinn_one               3.369e-02  1.839e-02   1.832 0.068051 .  \n",
       "afinn_two               1.273e-02  8.807e-03   1.445 0.149601    \n",
       "afinn_three            -2.882e-02  1.415e-02  -2.037 0.042635 *  \n",
       "afinn_neg_one          -6.499e-02  1.944e-02  -3.343 0.000951 ***\n",
       "sd.conf.pitch          -1.053e+00  6.047e-01  -1.741 0.082789 .  \n",
       "mean.spec.entropy      -1.769e+00  7.835e-01  -2.257 0.024814 *  \n",
       "sd.spec.entropy        -7.623e+00  3.492e+00  -2.183 0.029931 *  \n",
       "mean.val.apeak          6.654e+00  3.816e+00   1.744 0.082392 .  \n",
       "sd.val.apeak            1.455e+01  6.545e+00   2.222 0.027124 *  \n",
       "mean.loc.apeak         -4.768e+01  2.876e+01  -1.658 0.098578 .  \n",
       "sd.loc.apeak            6.865e-01  2.832e-01   2.424 0.016046 *  \n",
       "sd.num.apeak           -2.854e-01  1.156e-01  -2.469 0.014214 *  \n",
       "mean.d.energy           3.938e+02  2.226e+02   1.769 0.078066 .  \n",
       "sd.d.energy            -2.310e+01  1.214e+01  -1.903 0.058146 .  \n",
       "avg.len.seg            -6.310e-02  2.697e-02  -2.339 0.020091 *  \n",
       "time.speaking          -3.893e+00  1.965e+00  -1.981 0.048612 *  \n",
       "num.turns              -4.114e+00  2.980e+00  -1.381 0.168572    \n",
       "hogv.entropy            4.226e-01  1.917e-01   2.205 0.028363 *  \n",
       "hogv.median             1.622e+00  5.664e-01   2.865 0.004517 ** \n",
       "I(anticipation^2)      -4.184e-04  3.110e-04  -1.345 0.179786    \n",
       "I(disgust^2)            2.864e-03  9.223e-04   3.105 0.002115 ** \n",
       "I(fear^2)              -5.998e-04  4.331e-04  -1.385 0.167228    \n",
       "I(negative^2)           5.921e-04  2.566e-04   2.307 0.021831 *  \n",
       "I(sadness^2)           -1.570e-03  5.695e-04  -2.757 0.006259 ** \n",
       "I(trust^2)              3.123e-04  2.436e-04   1.282 0.200933    \n",
       "I(nrc_neutral^2)        1.601e-05  8.223e-06   1.948 0.052559 .  \n",
       "I(prop_longwords^2)    -2.074e+01  7.347e+00  -2.823 0.005133 ** \n",
       "I(sent_length^2)       -6.679e-05  3.310e-05  -2.018 0.044615 *  \n",
       "I(prop_longsent^2)      5.447e+00  2.206e+00   2.469 0.014185 *  \n",
       "I(afinn_neg_four^2)     2.481e-03  1.266e-03   1.960 0.051026 .  \n",
       "I(afinn_neg_three^2)    4.145e-03  1.980e-03   2.093 0.037339 *  \n",
       "I(afinn_one^2)         -1.709e-03  6.026e-04  -2.837 0.004919 ** \n",
       "I(afinn_two^2)         -2.842e-04  1.380e-04  -2.060 0.040365 *  \n",
       "I(afinn_neutral^2)     -1.539e-05  7.805e-06  -1.971 0.049755 *  \n",
       "I(afinn_neg_five^2)    -6.818e-02  2.542e-02  -2.683 0.007778 ** \n",
       "I(mean.pitch^2)        -4.762e-06  2.714e-06  -1.754 0.080580 .  \n",
       "I(sd.pitch^2)          -6.694e-01  4.862e-01  -1.377 0.169724    \n",
       "I(mean.conf.pitch^2)    5.312e-01  1.369e-01   3.880 0.000133 ***\n",
       "I(mean.spec.entropy^2)  2.686e-01  1.375e-01   1.954 0.051779 .  \n",
       "I(sd.spec.entropy^2)    9.911e+00  4.605e+00   2.152 0.032311 *  \n",
       "I(mean.val.apeak^2)    -5.040e+00  3.337e+00  -1.510 0.132247    \n",
       "I(sd.val.apeak^2)      -2.686e+01  1.426e+01  -1.884 0.060731 .  \n",
       "I(mean.loc.apeak^2)     1.053e+03  4.610e+02   2.285 0.023122 *  \n",
       "I(mean.num.apeak^2)    -7.283e-03  2.591e-03  -2.811 0.005316 ** \n",
       "I(sd.num.apeak^2)       2.173e-02  1.149e-02   1.892 0.059659 .  \n",
       "I(sd.d.energy^2)        9.042e+02  3.272e+02   2.763 0.006131 ** \n",
       "I(time.speaking^2)      4.339e+00  1.696e+00   2.558 0.011096 *  \n",
       "I(num.turns^2)          7.439e+00  4.125e+00   1.803 0.072515 .  \n",
       "I(hogv.entropy^2)      -4.998e-02  2.065e-02  -2.420 0.016204 *  \n",
       "I(hogv.cogC^2)         -2.004e-05  6.918e-06  -2.896 0.004099 ** \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.6404 on 258 degrees of freedom\n",
       "  (80 observations deleted due to missingness)\n",
       "Multiple R-squared:  0.5862,\tAdjusted R-squared:  0.4836 \n",
       "F-statistic: 5.711 on 64 and 258 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a model with a second-order polinomial term for each of the predictors except gender (full model)\n",
    "fit_agr = lm(Agr ~ . - Extr - Cons - Emot - Open + I(anger^2) + I(anticipation^2) +\n",
    "                I(disgust^2) + I(fear^2) + I(joy^2) +\n",
    "                I(negative^2) + I(positive^2) + I(sadness^2) +\n",
    "                I(surprise^2) + I(trust^2) + I(nrc_neutral^2) +\n",
    "                I(prop_longwords^2) + I(sent_length^2) + I(prop_longsent^2) + I(stopw_count^2) +\n",
    "                I(afinn_neg_four^2) + I(afinn_neg_three^2) + I(afinn_neg_two^2) +\n",
    "                I(afinn_one^2) + I(afinn_two^2) + I(afinn_three^2) +\n",
    "                I(afinn_four^2) + I(afinn_neutral^2) + I(afinn_neg_one^2) +\n",
    "                I(afinn_neg_five^2) + I(afinn_five^2) + I(mean.pitch^2) +      \n",
    "                I(sd.pitch^2) + I(mean.conf.pitch^2) + I(sd.conf.pitch^2) +    \n",
    "                I(mean.spec.entropy^2) + I(sd.spec.entropy^2) + I(mean.val.apeak^2) +   \n",
    "                I(sd.val.apeak^2) + I(mean.loc.apeak^2) + I(sd.loc.apeak^2) +     \n",
    "                I(mean.num.apeak^2) + I(sd.num.apeak^2) + I(mean.energy^2) +      \n",
    "                I(sd.energy^2) + I(mean.d.energy^2) + I(sd.d.energy^2) +      \n",
    "                I(avg.voiced.seg^2) + I(avg.len.seg^2) + I(time.speaking^2) +    \n",
    "                I(voice.rate^2) + I(num.turns^2) + I(hogv.entropy^2) +     \n",
    "                I(hogv.median^2) + I(hogv.cogR^2) + I(hogv.cogC^2),\n",
    "              data = vlogger_df[,-1])\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[2,1] <- sqrt(mean(fit_agr$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[2,2] <- length(fit_agr$coef) \n",
    "\n",
    "# Run Mixed model selection and save formula\n",
    "formula <- stepAIC(fit_agr, direction = \"both\", trace = F)\n",
    "\n",
    "# Recompute regression model with the paramters dropped\n",
    "fit_agr_step = lm(formula$call$formula, data = vlogger_df[,-1])\n",
    "\n",
    "# Summary of final model (after stepwise selection)\n",
    "summary(fit_agr_step)\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[2,3] <- sqrt(mean(fit_agr_step$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[2,4] <- length(fit_agr_step$coef) # 65 (vs 104 in the full model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b158a9aa",
   "metadata": {
    "papermill": {
     "duration": 0.042252,
     "end_time": "2023-09-18T11:36:00.031741",
     "exception": false,
     "start_time": "2023-09-18T11:35:59.989489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 3. Conscientiousness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db05d862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:00.086153Z",
     "iopub.status.busy": "2023-09-18T11:36:00.084622Z",
     "iopub.status.idle": "2023-09-18T11:36:10.863410Z",
     "shell.execute_reply": "2023-09-18T11:36:10.856875Z"
    },
    "papermill": {
     "duration": 10.811925,
     "end_time": "2023-09-18T11:36:10.869065",
     "exception": false,
     "start_time": "2023-09-18T11:36:00.057140",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = formula$call$formula, data = vlogger_df[, -1])\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.06618 -0.31199  0.01352  0.37116  1.63544 \n",
       "\n",
       "Coefficients:\n",
       "                       Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)           5.741e+00  1.021e+00   5.621 4.63e-08 ***\n",
       "disgust              -3.695e-02  1.529e-02  -2.416 0.016329 *  \n",
       "fear                  3.936e-02  1.258e-02   3.129 0.001943 ** \n",
       "joy                  -2.430e-02  1.130e-02  -2.151 0.032372 *  \n",
       "positive              6.383e-02  1.518e-02   4.205 3.53e-05 ***\n",
       "surprise              4.909e-02  2.640e-02   1.860 0.063965 .  \n",
       "trust                -3.993e-02  1.896e-02  -2.106 0.036070 *  \n",
       "nrc_neutral           1.290e-02  6.421e-03   2.009 0.045556 *  \n",
       "prop_longwords        8.017e+00  3.416e+00   2.347 0.019645 *  \n",
       "stopw_count          -1.039e-02  4.761e-03  -2.183 0.029883 *  \n",
       "afinn_neg_four       -2.619e-02  1.614e-02  -1.623 0.105762    \n",
       "afinn_neg_three      -2.912e-02  1.898e-02  -1.534 0.126049    \n",
       "afinn_two            -1.414e-02  8.849e-03  -1.598 0.111274    \n",
       "afinn_neutral        -1.199e-02  6.204e-03  -1.933 0.054231 .  \n",
       "afinn_neg_one        -5.255e-02  1.864e-02  -2.820 0.005153 ** \n",
       "afinn_five           -6.296e-01  3.338e-01  -1.886 0.060297 .  \n",
       "mean.conf.pitch      -1.976e+00  9.010e-01  -2.194 0.029084 *  \n",
       "mean.val.apeak        1.443e+00  5.330e-01   2.706 0.007222 ** \n",
       "sd.d.energy          -3.338e+01  1.143e+01  -2.920 0.003792 ** \n",
       "avg.voiced.seg        4.556e-01  3.453e-01   1.319 0.188113    \n",
       "avg.len.seg          -3.996e-01  1.067e-01  -3.746 0.000218 ***\n",
       "num.turns            -1.147e+01  3.035e+00  -3.780 0.000192 ***\n",
       "hogv.entropy          3.909e-01  1.881e-01   2.079 0.038550 *  \n",
       "hogv.median           1.115e+00  5.516e-01   2.020 0.044294 *  \n",
       "hogv.cogC            -7.160e-03  2.068e-03  -3.462 0.000621 ***\n",
       "I(negative^2)         7.110e-04  2.691e-04   2.642 0.008700 ** \n",
       "I(positive^2)        -2.701e-04  1.459e-04  -1.851 0.065205 .  \n",
       "I(surprise^2)        -1.885e-03  1.181e-03  -1.596 0.111706    \n",
       "I(trust^2)            4.706e-04  2.992e-04   1.573 0.116894    \n",
       "I(prop_longwords^2)  -1.404e+01  6.772e+00  -2.074 0.039038 *  \n",
       "I(sent_length^2)     -1.576e-05  9.747e-06  -1.617 0.106931    \n",
       "I(prop_longsent^2)    2.588e+00  1.138e+00   2.274 0.023729 *  \n",
       "I(stopw_count^2)      5.045e-05  2.705e-05   1.865 0.063228 .  \n",
       "I(afinn_neg_two^2)   -2.603e-03  8.402e-04  -3.098 0.002148 ** \n",
       "I(afinn_one^2)       -7.726e-04  3.401e-04  -2.272 0.023849 *  \n",
       "I(sd.pitch^2)        -8.151e-01  4.773e-01  -1.708 0.088787 .  \n",
       "I(mean.conf.pitch^2)  6.867e-01  3.191e-01   2.152 0.032266 *  \n",
       "I(sd.val.apeak^2)     7.970e+00  2.664e+00   2.992 0.003025 ** \n",
       "I(sd.d.energy^2)      8.861e+02  2.928e+02   3.027 0.002704 ** \n",
       "I(avg.voiced.seg^2)  -4.703e-02  3.545e-02  -1.327 0.185711    \n",
       "I(avg.len.seg^2)      9.097e-03  2.833e-03   3.211 0.001479 ** \n",
       "I(time.speaking^2)    2.431e+00  4.361e-01   5.574 5.89e-08 ***\n",
       "I(num.turns^2)        1.521e+01  3.952e+00   3.849 0.000147 ***\n",
       "I(hogv.entropy^2)    -4.897e-02  2.011e-02  -2.436 0.015500 *  \n",
       "I(anger^2)           -1.485e-03  5.938e-04  -2.501 0.012963 *  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.639 on 278 degrees of freedom\n",
       "  (80 observations deleted due to missingness)\n",
       "Multiple R-squared:  0.4344,\tAdjusted R-squared:  0.3449 \n",
       "F-statistic: 4.853 on 44 and 278 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a model with a second-order polinomial term for each of the predictors except gender (full model)\n",
    "fit_cons = lm(Cons ~ . - Extr - Agr - Emot - Open + I(anger^2) + I(anger^2) + I(anticipation^2) +\n",
    "                I(disgust^2) + I(fear^2) + I(joy^2) +\n",
    "                I(negative^2) + I(positive^2) + I(sadness^2) +\n",
    "                I(surprise^2) + I(trust^2) + I(nrc_neutral^2) +\n",
    "                I(prop_longwords^2) + I(sent_length^2) + I(prop_longsent^2) + I(stopw_count^2) +\n",
    "                I(afinn_neg_four^2) + I(afinn_neg_three^2) + I(afinn_neg_two^2) +\n",
    "                I(afinn_one^2) + I(afinn_two^2) + I(afinn_three^2) +\n",
    "                I(afinn_four^2) + I(afinn_neutral^2) + I(afinn_neg_one^2) +\n",
    "                I(afinn_neg_five^2) + I(afinn_five^2) + I(mean.pitch^2) +      \n",
    "                I(sd.pitch^2) + I(mean.conf.pitch^2) + I(sd.conf.pitch^2) +    \n",
    "                I(mean.spec.entropy^2) + I(sd.spec.entropy^2) + I(mean.val.apeak^2) +   \n",
    "                I(sd.val.apeak^2) + I(mean.loc.apeak^2) + I(sd.loc.apeak^2) +     \n",
    "                I(mean.num.apeak^2) + I(sd.num.apeak^2) + I(mean.energy^2) +      \n",
    "                I(sd.energy^2) + I(mean.d.energy^2) + I(sd.d.energy^2) +      \n",
    "                I(avg.voiced.seg^2) + I(avg.len.seg^2) + I(time.speaking^2) +    \n",
    "                I(voice.rate^2) + I(num.turns^2) + I(hogv.entropy^2) +     \n",
    "                I(hogv.median^2) + I(hogv.cogR^2) + I(hogv.cogC^2),\n",
    "              data = vlogger_df[,-1])\n",
    "\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[3,1] <- sqrt(mean(fit_cons$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[3,2] <- length(fit_cons$coef) \n",
    "\n",
    "# Run Mixed model selection and save formula\n",
    "formula <- stepAIC(fit_cons, direction = \"both\", trace = F)\n",
    "\n",
    "# Recompute regression model with the paramters dropped\n",
    "fit_cons_step = lm(formula$call$formula, data = vlogger_df[,-1])\n",
    "\n",
    "# Summary of final model (after stepwise selection)\n",
    "summary(fit_cons_step)\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[3,3] <- sqrt(mean(fit_cons_step$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[3,4] <- length(fit_cons_step$coef) # 45 (vs 104 in the full model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe235e0",
   "metadata": {
    "papermill": {
     "duration": 0.037605,
     "end_time": "2023-09-18T11:36:10.962690",
     "exception": false,
     "start_time": "2023-09-18T11:36:10.925085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 4. Emotional Stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b793eb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:11.018718Z",
     "iopub.status.busy": "2023-09-18T11:36:11.017022Z",
     "iopub.status.idle": "2023-09-18T11:36:19.845690Z",
     "shell.execute_reply": "2023-09-18T11:36:19.834739Z"
    },
    "papermill": {
     "duration": 8.861431,
     "end_time": "2023-09-18T11:36:19.849968",
     "exception": false,
     "start_time": "2023-09-18T11:36:10.988537",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = formula$call$formula, data = vlogger_df[, -1])\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.90270 -0.31031  0.05329  0.33973  1.26416 \n",
       "\n",
       "Coefficients:\n",
       "                         Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)             9.526e+00  2.407e+00   3.957 9.75e-05 ***\n",
       "joy                    -3.180e-02  1.676e-02  -1.897 0.058905 .  \n",
       "positive                1.898e-02  8.328e-03   2.279 0.023468 *  \n",
       "surprise                5.751e-02  2.601e-02   2.211 0.027870 *  \n",
       "prop_longwords          1.354e+01  3.337e+00   4.059 6.49e-05 ***\n",
       "afinn_neg_four         -4.523e-02  1.587e-02  -2.850 0.004717 ** \n",
       "afinn_neg_three        -5.317e-02  1.645e-02  -3.231 0.001389 ** \n",
       "afinn_neg_two          -3.414e-02  1.365e-02  -2.501 0.012978 *  \n",
       "afinn_two               1.645e-02  7.932e-03   2.074 0.039053 *  \n",
       "afinn_neg_five         -1.775e-01  9.506e-02  -1.867 0.062990 .  \n",
       "afinn_five             -8.284e-01  3.405e-01  -2.433 0.015627 *  \n",
       "mean.pitch              1.813e-02  5.512e-03   3.289 0.001143 ** \n",
       "sd.pitch                4.594e+00  1.731e+00   2.654 0.008438 ** \n",
       "sd.conf.pitch          -4.507e+00  1.935e+00  -2.329 0.020602 *  \n",
       "mean.spec.entropy      -3.139e+00  1.395e+00  -2.251 0.025213 *  \n",
       "sd.spec.entropy        -9.892e+00  3.167e+00  -3.124 0.001984 ** \n",
       "mean.val.apeak          1.215e+00  6.879e-01   1.766 0.078548 .  \n",
       "sd.val.apeak            3.466e+00  1.459e+00   2.375 0.018259 *  \n",
       "mean.loc.apeak         -3.563e+01  2.714e+01  -1.313 0.190356    \n",
       "sd.num.apeak            2.148e-01  1.116e-01   1.924 0.055431 .  \n",
       "mean.energy             1.217e+01  4.055e+00   3.002 0.002937 ** \n",
       "mean.d.energy           8.014e+02  3.535e+02   2.267 0.024215 *  \n",
       "sd.d.energy            -3.620e+01  1.227e+01  -2.950 0.003463 ** \n",
       "avg.len.seg            -3.024e-01  1.114e-01  -2.714 0.007094 ** \n",
       "num.turns              -1.189e+01  3.295e+00  -3.610 0.000366 ***\n",
       "hogv.entropy            3.475e-01  1.815e-01   1.915 0.056602 .  \n",
       "hogv.median             2.110e+00  5.352e-01   3.943 0.000103 ***\n",
       "hogv.cogR               6.591e-03  2.089e-03   3.155 0.001790 ** \n",
       "hogv.cogC              -5.621e-03  2.014e-03  -2.791 0.005637 ** \n",
       "I(disgust^2)           -7.193e-04  5.475e-04  -1.314 0.190014    \n",
       "I(fear^2)               6.921e-04  2.678e-04   2.584 0.010302 *  \n",
       "I(joy^2)                5.248e-04  3.249e-04   1.616 0.107377    \n",
       "I(negative^2)           3.460e-04  1.917e-04   1.805 0.072136 .  \n",
       "I(sadness^2)           -1.131e-03  4.972e-04  -2.274 0.023745 *  \n",
       "I(surprise^2)          -2.151e-03  1.206e-03  -1.784 0.075628 .  \n",
       "I(nrc_neutral^2)        8.535e-06  5.822e-06   1.466 0.143803    \n",
       "I(prop_longwords^2)    -2.646e+01  6.644e+00  -3.982 8.82e-05 ***\n",
       "I(sent_length^2)       -2.515e-05  9.475e-06  -2.655 0.008418 ** \n",
       "I(prop_longsent^2)      2.944e+00  1.088e+00   2.706 0.007248 ** \n",
       "I(afinn_one^2)         -3.989e-04  2.966e-04  -1.345 0.179830    \n",
       "I(afinn_two^2)         -2.410e-04  1.075e-04  -2.242 0.025781 *  \n",
       "I(afinn_neutral^2)     -8.529e-06  5.456e-06  -1.563 0.119220    \n",
       "I(afinn_neg_one^2)     -7.675e-03  1.547e-03  -4.961 1.26e-06 ***\n",
       "I(mean.pitch^2)        -3.753e-05  1.126e-05  -3.333 0.000982 ***\n",
       "I(sd.pitch^2)          -5.331e+00  1.843e+00  -2.893 0.004129 ** \n",
       "I(mean.conf.pitch^2)    1.602e-01  1.146e-01   1.398 0.163193    \n",
       "I(sd.conf.pitch^2)      3.918e+00  1.887e+00   2.077 0.038803 *  \n",
       "I(mean.spec.entropy^2)  3.786e-01  2.116e-01   1.789 0.074737 .  \n",
       "I(sd.spec.entropy^2)    1.126e+01  4.217e+00   2.671 0.008035 ** \n",
       "I(mean.loc.apeak^2)     9.293e+02  4.223e+02   2.200 0.028639 *  \n",
       "I(mean.num.apeak^2)    -4.130e-03  2.398e-03  -1.722 0.086216 .  \n",
       "I(sd.num.apeak^2)      -2.217e-02  1.224e-02  -1.812 0.071132 .  \n",
       "I(mean.d.energy^2)      3.225e+05  1.558e+05   2.069 0.039488 *  \n",
       "I(avg.voiced.seg^2)    -5.238e-02  2.616e-02  -2.002 0.046255 *  \n",
       "I(avg.len.seg^2)        6.276e-03  2.957e-03   2.122 0.034756 *  \n",
       "I(time.speaking^2)      1.687e+00  4.381e-01   3.850 0.000148 ***\n",
       "I(num.turns^2)          1.572e+01  4.244e+00   3.705 0.000257 ***\n",
       "I(hogv.entropy^2)      -4.482e-02  1.940e-02  -2.310 0.021662 *  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.6015 on 265 degrees of freedom\n",
       "  (80 observations deleted due to missingness)\n",
       "Multiple R-squared:  0.4997,\tAdjusted R-squared:  0.3921 \n",
       "F-statistic: 4.644 on 57 and 265 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a model with a second-order polinomial term for each of the predictors except gender (full model)\n",
    "fit_emot = lm(Emot ~ . - Extr - Agr - Cons - Open + I(anger^2) + I(anticipation^2) +\n",
    "                I(disgust^2) + I(fear^2) + I(joy^2) +\n",
    "                I(negative^2) + I(positive^2) + I(sadness^2) +\n",
    "                I(surprise^2) + I(trust^2) + I(nrc_neutral^2) +\n",
    "                I(prop_longwords^2) + I(sent_length^2) + I(prop_longsent^2) + I(stopw_count^2) +\n",
    "                I(afinn_neg_four^2) + I(afinn_neg_three^2) + I(afinn_neg_two^2) +\n",
    "                I(afinn_one^2) + I(afinn_two^2) + I(afinn_three^2) +\n",
    "                I(afinn_four^2) + I(afinn_neutral^2) + I(afinn_neg_one^2) +\n",
    "                I(afinn_neg_five^2) + I(afinn_five^2) + I(mean.pitch^2) +      \n",
    "                I(sd.pitch^2) + I(mean.conf.pitch^2) + I(sd.conf.pitch^2) +    \n",
    "                I(mean.spec.entropy^2) + I(sd.spec.entropy^2) + I(mean.val.apeak^2) +   \n",
    "                I(sd.val.apeak^2) + I(mean.loc.apeak^2) + I(sd.loc.apeak^2) +     \n",
    "                I(mean.num.apeak^2) + I(sd.num.apeak^2) + I(mean.energy^2) +      \n",
    "                I(sd.energy^2) + I(mean.d.energy^2) + I(sd.d.energy^2) +      \n",
    "                I(avg.voiced.seg^2) + I(avg.len.seg^2) + I(time.speaking^2) +    \n",
    "                I(voice.rate^2) + I(num.turns^2) + I(hogv.entropy^2) +     \n",
    "                I(hogv.median^2) + I(hogv.cogR^2) + I(hogv.cogC^2),\n",
    "              data = vlogger_df[,-1])\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[4,1] <- sqrt(mean(fit_emot$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[4,2] <- length(fit_emot$coef) \n",
    "\n",
    "# Run Mixed model selection and save formula\n",
    "formula <- stepAIC(fit_emot, direction = \"both\", trace = F)\n",
    "\n",
    "# Recompute regression model with the paramters dropped\n",
    "fit_emot_step = lm(formula$call$formula, data = vlogger_df[,-1])\n",
    "\n",
    "# Summary of final model (after stepwise selection)\n",
    "summary(fit_emot_step)\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[4,3] <- sqrt(mean(fit_emot_step$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[4,4] <- length(fit_emot_step$coef) # 58 (vs 104 in the full model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6a1a1",
   "metadata": {
    "papermill": {
     "duration": 0.026883,
     "end_time": "2023-09-18T11:36:19.934888",
     "exception": false,
     "start_time": "2023-09-18T11:36:19.908005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 5. Openness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddec3372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:19.993943Z",
     "iopub.status.busy": "2023-09-18T11:36:19.992049Z",
     "iopub.status.idle": "2023-09-18T11:36:31.004185Z",
     "shell.execute_reply": "2023-09-18T11:36:30.999088Z"
    },
    "papermill": {
     "duration": 11.046774,
     "end_time": "2023-09-18T11:36:31.008435",
     "exception": false,
     "start_time": "2023-09-18T11:36:19.961661",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = formula$call$formula, data = vlogger_df[, -1])\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.34058 -0.40457  0.00492  0.41031  1.56884 \n",
       "\n",
       "Coefficients:\n",
       "                       Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)           3.962e+00  1.027e+00   3.858 0.000142 ***\n",
       "genderMale            4.185e-01  1.094e-01   3.825 0.000161 ***\n",
       "anger                 4.852e-02  2.172e-02   2.233 0.026307 *  \n",
       "negative             -2.931e-02  1.514e-02  -1.936 0.053878 .  \n",
       "positive              1.473e-02  4.098e-03   3.594 0.000385 ***\n",
       "surprise             -1.674e-02  1.067e-02  -1.570 0.117574    \n",
       "prop_longwords        9.068e+00  3.391e+00   2.674 0.007933 ** \n",
       "sent_length           1.080e-02  7.362e-03   1.467 0.143432    \n",
       "prop_longsent        -2.266e+00  1.240e+00  -1.828 0.068567 .  \n",
       "afinn_four            4.697e-02  1.729e-02   2.717 0.006998 ** \n",
       "afinn_neg_five        4.049e-01  2.421e-01   1.673 0.095538 .  \n",
       "afinn_five           -4.920e-01  3.079e-01  -1.598 0.111130    \n",
       "mean.pitch            1.389e-02  5.182e-03   2.681 0.007782 ** \n",
       "sd.conf.pitch        -2.438e+00  1.787e+00  -1.364 0.173750    \n",
       "sd.spec.entropy      -5.291e+00  2.720e+00  -1.945 0.052777 .  \n",
       "sd.val.apeak          3.825e+00  1.564e+00   2.445 0.015085 *  \n",
       "mean.num.apeak       -9.233e-02  3.878e-02  -2.381 0.017952 *  \n",
       "sd.num.apeak          1.894e-01  1.008e-01   1.878 0.061427 .  \n",
       "avg.len.seg          -8.205e-02  2.462e-02  -3.332 0.000978 ***\n",
       "num.turns            -5.504e+00  2.386e+00  -2.307 0.021783 *  \n",
       "hogv.cogC            -3.067e-03  1.997e-03  -1.536 0.125755    \n",
       "I(anger^2)           -2.149e-03  8.764e-04  -2.453 0.014794 *  \n",
       "I(negative^2)         9.993e-04  3.941e-04   2.536 0.011771 *  \n",
       "I(sadness^2)         -8.838e-04  4.547e-04  -1.944 0.052912 .  \n",
       "I(nrc_neutral^2)     -1.737e-06  5.591e-07  -3.106 0.002092 ** \n",
       "I(prop_longwords^2)  -1.904e+01  6.717e+00  -2.835 0.004917 ** \n",
       "I(sent_length^2)     -4.377e-05  3.040e-05  -1.440 0.151024    \n",
       "I(prop_longsent^2)    4.628e+00  2.023e+00   2.287 0.022919 *  \n",
       "I(afinn_neg_four^2)  -1.608e-03  6.126e-04  -2.625 0.009136 ** \n",
       "I(afinn_neg_three^2) -1.200e-03  8.848e-04  -1.356 0.176063    \n",
       "I(afinn_neg_five^2)  -1.227e-01  6.111e-02  -2.008 0.045644 *  \n",
       "I(mean.pitch^2)      -2.629e-05  1.045e-05  -2.517 0.012403 *  \n",
       "I(sd.pitch^2)        -9.549e-01  4.486e-01  -2.129 0.034155 *  \n",
       "I(sd.conf.pitch^2)    2.873e+00  1.744e+00   1.647 0.100688    \n",
       "I(sd.spec.entropy^2)  5.714e+00  3.700e+00   1.544 0.123637    \n",
       "I(mean.val.apeak^2)   3.849e+00  2.099e+00   1.834 0.067658 .  \n",
       "I(mean.loc.apeak^2)   5.022e+02  1.660e+02   3.025 0.002717 ** \n",
       "I(sd.num.apeak^2)    -2.314e-02  1.028e-02  -2.251 0.025135 *  \n",
       "I(time.speaking^2)    1.016e+00  3.166e-01   3.209 0.001484 ** \n",
       "I(num.turns^2)        6.868e+00  3.471e+00   1.979 0.048818 *  \n",
       "I(hogv.median^2)      3.573e+00  6.803e-01   5.252 2.97e-07 ***\n",
       "mean.val.apeak       -3.654e+00  2.400e+00  -1.522 0.129124    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.612 on 281 degrees of freedom\n",
       "  (80 observations deleted due to missingness)\n",
       "Multiple R-squared:   0.35,\tAdjusted R-squared:  0.2552 \n",
       "F-statistic: 3.691 on 41 and 281 DF,  p-value: 4.823e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a model with a second-order polinomial term for each of the predictors except gender (full model)\n",
    "fit_open = lm(Open ~ . - Extr - Agr - Cons - Emot + I(anger^2) + I(anticipation^2) +\n",
    "                I(disgust^2) + I(fear^2) + I(joy^2) +\n",
    "                I(negative^2) + I(positive^2) + I(sadness^2) +\n",
    "                I(surprise^2) + I(trust^2) + I(nrc_neutral^2) +\n",
    "                I(prop_longwords^2) + I(sent_length^2) + I(prop_longsent^2) + I(stopw_count^2) +\n",
    "                I(afinn_neg_four^2) + I(afinn_neg_three^2) + I(afinn_neg_two^2) +\n",
    "                I(afinn_one^2) + I(afinn_two^2) + I(afinn_three^2) +\n",
    "                I(afinn_four^2) + I(afinn_neutral^2) + I(afinn_neg_one^2) +\n",
    "                I(afinn_neg_five^2) + I(afinn_five^2) + I(mean.pitch^2) +      \n",
    "                I(sd.pitch^2) + I(mean.conf.pitch^2) + I(sd.conf.pitch^2) +    \n",
    "                I(mean.spec.entropy^2) + I(sd.spec.entropy^2) + I(mean.val.apeak^2) +   \n",
    "                I(sd.val.apeak^2) + I(mean.loc.apeak^2) + I(sd.loc.apeak^2) +     \n",
    "                I(mean.num.apeak^2) + I(sd.num.apeak^2) + I(mean.energy^2) +      \n",
    "                I(sd.energy^2) + I(mean.d.energy^2) + I(sd.d.energy^2) +      \n",
    "                I(avg.voiced.seg^2) + I(avg.len.seg^2) + I(time.speaking^2) +    \n",
    "                I(voice.rate^2) + I(num.turns^2) + I(hogv.entropy^2) +     \n",
    "                I(hogv.median^2) + I(hogv.cogR^2) + I(hogv.cogC^2),\n",
    "              data = vlogger_df[,-1])\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[5,1] <- sqrt(mean(fit_open$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[5,2] <- length(fit_open$coef) \n",
    "\n",
    "# Run Mixed model selection and save formula\n",
    "formula <- stepAIC(fit_open, direction = \"both\", trace = F)\n",
    "\n",
    "# Recompute regression model with the paramters dropped\n",
    "fit_open_step = lm(formula$call$formula, data = vlogger_df[,-1])\n",
    "\n",
    "# Summary of final model (after stepwise selection)\n",
    "summary(fit_open_step)\n",
    "\n",
    "# RMSE (training data)\n",
    "table1[5,3] <- sqrt(mean(fit_open_step$residuals^2)) \n",
    "\n",
    "# Check number of coefficients in the model\n",
    "table1[5,4] <- length(fit_open_step$coef) # 42 (vs 104 in the full model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfad65",
   "metadata": {
    "papermill": {
     "duration": 0.04337,
     "end_time": "2023-09-18T11:36:31.107497",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.064127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Table 1. Comparison of baseline models and reduced models after stepwise selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9541f473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:31.163382Z",
     "iopub.status.busy": "2023-09-18T11:36:31.161719Z",
     "iopub.status.idle": "2023-09-18T11:36:31.188987Z",
     "shell.execute_reply": "2023-09-18T11:36:31.186630Z"
    },
    "papermill": {
     "duration": 0.058638,
     "end_time": "2023-09-18T11:36:31.192205",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.133567",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>RMSE.baseline</th><th scope=col>Number.Predictors.Baseline</th><th scope=col>RMSE.reduced</th><th scope=col>Number.Predictors.Reduced</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Extraversion</th><td>0.6090791</td><td>105</td><td>0.6462272</td><td>43</td></tr>\n",
       "\t<tr><th scope=row>Agreeableness</th><td>0.5598601</td><td>104</td><td>0.5723140</td><td>65</td></tr>\n",
       "\t<tr><th scope=row>Conscientiousness</th><td>0.5651502</td><td>104</td><td>0.5928525</td><td>45</td></tr>\n",
       "\t<tr><th scope=row>Emotional Stability</th><td>0.5265495</td><td>104</td><td>0.5448355</td><td>58</td></tr>\n",
       "\t<tr><th scope=row>Openness</th><td>0.5399606</td><td>104</td><td>0.5708363</td><td>42</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 4\n",
       "\\begin{tabular}{r|llll}\n",
       "  & RMSE.baseline & Number.Predictors.Baseline & RMSE.reduced & Number.Predictors.Reduced\\\\\n",
       "  & <dbl> & <int> & <dbl> & <int>\\\\\n",
       "\\hline\n",
       "\tExtraversion & 0.6090791 & 105 & 0.6462272 & 43\\\\\n",
       "\tAgreeableness & 0.5598601 & 104 & 0.5723140 & 65\\\\\n",
       "\tConscientiousness & 0.5651502 & 104 & 0.5928525 & 45\\\\\n",
       "\tEmotional Stability & 0.5265495 & 104 & 0.5448355 & 58\\\\\n",
       "\tOpenness & 0.5399606 & 104 & 0.5708363 & 42\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 4\n",
       "\n",
       "| <!--/--> | RMSE.baseline &lt;dbl&gt; | Number.Predictors.Baseline &lt;int&gt; | RMSE.reduced &lt;dbl&gt; | Number.Predictors.Reduced &lt;int&gt; |\n",
       "|---|---|---|---|---|\n",
       "| Extraversion | 0.6090791 | 105 | 0.6462272 | 43 |\n",
       "| Agreeableness | 0.5598601 | 104 | 0.5723140 | 65 |\n",
       "| Conscientiousness | 0.5651502 | 104 | 0.5928525 | 45 |\n",
       "| Emotional Stability | 0.5265495 | 104 | 0.5448355 | 58 |\n",
       "| Openness | 0.5399606 | 104 | 0.5708363 | 42 |\n",
       "\n"
      ],
      "text/plain": [
       "                    RMSE.baseline Number.Predictors.Baseline RMSE.reduced\n",
       "Extraversion        0.6090791     105                        0.6462272   \n",
       "Agreeableness       0.5598601     104                        0.5723140   \n",
       "Conscientiousness   0.5651502     104                        0.5928525   \n",
       "Emotional Stability 0.5265495     104                        0.5448355   \n",
       "Openness            0.5399606     104                        0.5708363   \n",
       "                    Number.Predictors.Reduced\n",
       "Extraversion        43                       \n",
       "Agreeableness       65                       \n",
       "Conscientiousness   45                       \n",
       "Emotional Stability 58                       \n",
       "Openness            42                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998f955",
   "metadata": {
    "_cell_guid": "d2def0aa-2cf4-47cf-b2f5-31477c6bffda",
    "_uuid": "04dc5694-5fba-4712-b6a0-5e8ea30ded86",
    "papermill": {
     "duration": 0.026298,
     "end_time": "2023-09-18T11:36:31.244881",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.218583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Making predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b840cbd",
   "metadata": {
    "_cell_guid": "2862139a-1fb4-4e1f-9136-e610955ef702",
    "_uuid": "f866665f-2c2a-40fa-bf08-fca006375058",
    "papermill": {
     "duration": 0.027135,
     "end_time": "2023-09-18T11:36:31.298894",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.271759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 The test set\n",
    "\n",
    "The test set are those `vlogId` that are missing in the personality scores data frame `pers`. They are the rows in `vlogger_df` for which the personality scores are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88bf5d26",
   "metadata": {
    "_cell_guid": "1afb86c1-4986-4908-96c3-4b143561363e",
    "_uuid": "f875910f-552b-4957-8f1a-d434ce9bdbd2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:31.355894Z",
     "iopub.status.busy": "2023-09-18T11:36:31.354319Z",
     "iopub.status.idle": "2023-09-18T11:36:31.417527Z",
     "shell.execute_reply": "2023-09-18T11:36:31.415604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.094897,
     "end_time": "2023-09-18T11:36:31.420178",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.325281",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 58</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>gender</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th><th scope=col>anger</th><th scope=col>anticipation</th><th scope=col>disgust</th><th scope=col>⋯</th><th scope=col>sd.d.energy</th><th scope=col>avg.voiced.seg</th><th scope=col>avg.len.seg</th><th scope=col>time.speaking</th><th scope=col>voice.rate</th><th scope=col>num.turns</th><th scope=col>hogv.entropy</th><th scope=col>hogv.median</th><th scope=col>hogv.cogR</th><th scope=col>hogv.cogC</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG8 </td><td>Female</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>12</td><td>2</td><td>⋯</td><td>0.01587400</td><td>0.16954</td><td>0.84412</td><td>0.46439</td><td>0.056864</td><td>0.55015</td><td>7.612877</td><td>0.418219</td><td>123</td><td>178</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG15</td><td>Male  </td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>8</td><td>23</td><td>3</td><td>⋯</td><td>0.00169930</td><td>0.41882</td><td>2.14720</td><td>0.71592</td><td>0.031669</td><td>0.33342</td><td>4.199742</td><td>0.003863</td><td>100</td><td>165</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG18</td><td>Male  </td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>2</td><td> 3</td><td>1</td><td>⋯</td><td>0.01954900</td><td>0.23584</td><td>1.70300</td><td>0.69587</td><td>0.046223</td><td>0.40860</td><td>7.107432</td><td>0.468896</td><td>123</td><td>157</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG22</td><td>Female</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>0</td><td> 1</td><td>1</td><td>⋯</td><td>0.01786100</td><td>0.35004</td><td>1.50870</td><td>0.42792</td><td>0.035558</td><td>0.28364</td><td>6.713452</td><td>0.227571</td><td>127</td><td>161</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG28</td><td>Male  </td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>1</td><td> 7</td><td>1</td><td>⋯</td><td>0.00062237</td><td>0.33089</td><td>1.56000</td><td>0.52014</td><td>0.036923</td><td>0.33342</td><td>2.880514</td><td>0.013793</td><td>121</td><td>158</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG29</td><td>Female</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>4</td><td>14</td><td>3</td><td>⋯</td><td>0.00377260</td><td>0.23339</td><td>2.24380</td><td>0.63590</td><td>0.045721</td><td>0.28341</td><td>5.036860</td><td>0.026627</td><td>155</td><td>190</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 58\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & vlogId & gender & Extr & Agr & Cons & Emot & Open & anger & anticipation & disgust & ⋯ & sd.d.energy & avg.voiced.seg & avg.len.seg & time.speaking & voice.rate & num.turns & hogv.entropy & hogv.median & hogv.cogR & hogv.cogC\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <int> & <int> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG8  & Female & NA & NA & NA & NA & NA & 3 & 12 & 2 & ⋯ & 0.01587400 & 0.16954 & 0.84412 & 0.46439 & 0.056864 & 0.55015 & 7.612877 & 0.418219 & 123 & 178\\\\\n",
       "\t2 & VLOG15 & Male   & NA & NA & NA & NA & NA & 8 & 23 & 3 & ⋯ & 0.00169930 & 0.41882 & 2.14720 & 0.71592 & 0.031669 & 0.33342 & 4.199742 & 0.003863 & 100 & 165\\\\\n",
       "\t3 & VLOG18 & Male   & NA & NA & NA & NA & NA & 2 &  3 & 1 & ⋯ & 0.01954900 & 0.23584 & 1.70300 & 0.69587 & 0.046223 & 0.40860 & 7.107432 & 0.468896 & 123 & 157\\\\\n",
       "\t4 & VLOG22 & Female & NA & NA & NA & NA & NA & 0 &  1 & 1 & ⋯ & 0.01786100 & 0.35004 & 1.50870 & 0.42792 & 0.035558 & 0.28364 & 6.713452 & 0.227571 & 127 & 161\\\\\n",
       "\t5 & VLOG28 & Male   & NA & NA & NA & NA & NA & 1 &  7 & 1 & ⋯ & 0.00062237 & 0.33089 & 1.56000 & 0.52014 & 0.036923 & 0.33342 & 2.880514 & 0.013793 & 121 & 158\\\\\n",
       "\t6 & VLOG29 & Female & NA & NA & NA & NA & NA & 4 & 14 & 3 & ⋯ & 0.00377260 & 0.23339 & 2.24380 & 0.63590 & 0.045721 & 0.28341 & 5.036860 & 0.026627 & 155 & 190\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 58\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | gender &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; | anger &lt;int&gt; | anticipation &lt;int&gt; | disgust &lt;int&gt; | ⋯ ⋯ | sd.d.energy &lt;dbl&gt; | avg.voiced.seg &lt;dbl&gt; | avg.len.seg &lt;dbl&gt; | time.speaking &lt;dbl&gt; | voice.rate &lt;dbl&gt; | num.turns &lt;dbl&gt; | hogv.entropy &lt;dbl&gt; | hogv.median &lt;dbl&gt; | hogv.cogR &lt;dbl&gt; | hogv.cogC &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | VLOG8  | Female | NA | NA | NA | NA | NA | 3 | 12 | 2 | ⋯ | 0.01587400 | 0.16954 | 0.84412 | 0.46439 | 0.056864 | 0.55015 | 7.612877 | 0.418219 | 123 | 178 |\n",
       "| 2 | VLOG15 | Male   | NA | NA | NA | NA | NA | 8 | 23 | 3 | ⋯ | 0.00169930 | 0.41882 | 2.14720 | 0.71592 | 0.031669 | 0.33342 | 4.199742 | 0.003863 | 100 | 165 |\n",
       "| 3 | VLOG18 | Male   | NA | NA | NA | NA | NA | 2 |  3 | 1 | ⋯ | 0.01954900 | 0.23584 | 1.70300 | 0.69587 | 0.046223 | 0.40860 | 7.107432 | 0.468896 | 123 | 157 |\n",
       "| 4 | VLOG22 | Female | NA | NA | NA | NA | NA | 0 |  1 | 1 | ⋯ | 0.01786100 | 0.35004 | 1.50870 | 0.42792 | 0.035558 | 0.28364 | 6.713452 | 0.227571 | 127 | 161 |\n",
       "| 5 | VLOG28 | Male   | NA | NA | NA | NA | NA | 1 |  7 | 1 | ⋯ | 0.00062237 | 0.33089 | 1.56000 | 0.52014 | 0.036923 | 0.33342 | 2.880514 | 0.013793 | 121 | 158 |\n",
       "| 6 | VLOG29 | Female | NA | NA | NA | NA | NA | 4 | 14 | 3 | ⋯ | 0.00377260 | 0.23339 | 2.24380 | 0.63590 | 0.045721 | 0.28341 | 5.036860 | 0.026627 | 155 | 190 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId gender Extr Agr Cons Emot Open anger anticipation disgust ⋯\n",
       "1 VLOG8  Female NA   NA  NA   NA   NA   3     12           2       ⋯\n",
       "2 VLOG15 Male   NA   NA  NA   NA   NA   8     23           3       ⋯\n",
       "3 VLOG18 Male   NA   NA  NA   NA   NA   2      3           1       ⋯\n",
       "4 VLOG22 Female NA   NA  NA   NA   NA   0      1           1       ⋯\n",
       "5 VLOG28 Male   NA   NA  NA   NA   NA   1      7           1       ⋯\n",
       "6 VLOG29 Female NA   NA  NA   NA   NA   4     14           3       ⋯\n",
       "  sd.d.energy avg.voiced.seg avg.len.seg time.speaking voice.rate num.turns\n",
       "1 0.01587400  0.16954        0.84412     0.46439       0.056864   0.55015  \n",
       "2 0.00169930  0.41882        2.14720     0.71592       0.031669   0.33342  \n",
       "3 0.01954900  0.23584        1.70300     0.69587       0.046223   0.40860  \n",
       "4 0.01786100  0.35004        1.50870     0.42792       0.035558   0.28364  \n",
       "5 0.00062237  0.33089        1.56000     0.52014       0.036923   0.33342  \n",
       "6 0.00377260  0.23339        2.24380     0.63590       0.045721   0.28341  \n",
       "  hogv.entropy hogv.median hogv.cogR hogv.cogC\n",
       "1 7.612877     0.418219    123       178      \n",
       "2 4.199742     0.003863    100       165      \n",
       "3 7.107432     0.468896    123       157      \n",
       "4 6.713452     0.227571    127       161      \n",
       "5 2.880514     0.013793    121       158      \n",
       "6 5.036860     0.026627    155       190      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset_vloggers = vlogger_df %>% \n",
    "    filter(is.na(Extr))\n",
    "\n",
    "head(testset_vloggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423fb9d",
   "metadata": {
    "_cell_guid": "560128db-06d2-493a-9b78-9fa2ff25881f",
    "_uuid": "d4c0a598-8241-4d91-a60b-cc964318422d",
    "papermill": {
     "duration": 0.027451,
     "end_time": "2023-09-18T11:36:31.474864",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.447413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4aac774a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:31.534164Z",
     "iopub.status.busy": "2023-09-18T11:36:31.532472Z",
     "iopub.status.idle": "2023-09-18T11:36:31.611507Z",
     "shell.execute_reply": "2023-09-18T11:36:31.609394Z"
    },
    "papermill": {
     "duration": 0.111115,
     "end_time": "2023-09-18T11:36:31.614346",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.503231",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5.706351</td><td>5.225881</td><td>4.103566</td><td>5.052406</td><td>4.946759</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>3.926754</td><td>4.401509</td><td>4.911829</td><td>5.128201</td><td>4.561287</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>5.897689</td><td>5.167584</td><td>4.563131</td><td>5.283955</td><td>5.677221</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.532928</td><td>3.096872</td><td>3.022277</td><td>2.616432</td><td>3.723506</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>3.543585</td><td>3.790763</td><td>4.133493</td><td>3.860960</td><td>4.145796</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5.099636</td><td>4.409763</td><td>4.219668</td><td>5.049241</td><td>4.453881</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Extr & Agr & Cons & Emot & Open\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 5.706351 & 5.225881 & 4.103566 & 5.052406 & 4.946759\\\\\n",
       "\t2 & 3.926754 & 4.401509 & 4.911829 & 5.128201 & 4.561287\\\\\n",
       "\t3 & 5.897689 & 5.167584 & 4.563131 & 5.283955 & 5.677221\\\\\n",
       "\t4 & 4.532928 & 3.096872 & 3.022277 & 2.616432 & 3.723506\\\\\n",
       "\t5 & 3.543585 & 3.790763 & 4.133493 & 3.860960 & 4.145796\\\\\n",
       "\t6 & 5.099636 & 4.409763 & 4.219668 & 5.049241 & 4.453881\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 5\n",
       "\n",
       "| <!--/--> | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 5.706351 | 5.225881 | 4.103566 | 5.052406 | 4.946759 |\n",
       "| 2 | 3.926754 | 4.401509 | 4.911829 | 5.128201 | 4.561287 |\n",
       "| 3 | 5.897689 | 5.167584 | 4.563131 | 5.283955 | 5.677221 |\n",
       "| 4 | 4.532928 | 3.096872 | 3.022277 | 2.616432 | 3.723506 |\n",
       "| 5 | 3.543585 | 3.790763 | 4.133493 | 3.860960 | 4.145796 |\n",
       "| 6 | 5.099636 | 4.409763 | 4.219668 | 5.049241 | 4.453881 |\n",
       "\n"
      ],
      "text/plain": [
       "  Extr     Agr      Cons     Emot     Open    \n",
       "1 5.706351 5.225881 4.103566 5.052406 4.946759\n",
       "2 3.926754 4.401509 4.911829 5.128201 4.561287\n",
       "3 5.897689 5.167584 4.563131 5.283955 5.677221\n",
       "4 4.532928 3.096872 3.022277 2.616432 3.723506\n",
       "5 3.543585 3.790763 4.133493 3.860960 4.145796\n",
       "6 5.099636 4.409763 4.219668 5.049241 4.453881"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions for each personality score separately\n",
    "pred_extr = predict(fit_extr_step, new = testset_vloggers[, -c(4,5,6,7)])\n",
    "pred_agr = predict(fit_agr_step, new = testset_vloggers[, -c(3,5,6,7)])\n",
    "pred_cons = predict(fit_cons_step, new = testset_vloggers[, -c(3,4,6,7)])\n",
    "pred_emot = predict(fit_emot_step, new = testset_vloggers[, -c(3,4,5,7)])\n",
    "pred_open = predict(fit_open_step, new = testset_vloggers[, -c(3,4,5,6)])\n",
    "\n",
    "# Merge predictions into a data frame\n",
    "pred = cbind(\n",
    "    Extr = pred_extr,\n",
    "    Agr = pred_agr,\n",
    "    Cons = pred_cons,\n",
    "    Emot = pred_emot,\n",
    "    Open = pred_open\n",
    "    ) %>%\n",
    "    as.data.frame()\n",
    "\n",
    "head(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbc47438",
   "metadata": {
    "_cell_guid": "873a1bde-0f8e-446d-8153-5681e6d27134",
    "_uuid": "c3543421-d7bd-42ea-a880-23825de5f8bc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:31.672969Z",
     "iopub.status.busy": "2023-09-18T11:36:31.671145Z",
     "iopub.status.idle": "2023-09-18T11:36:31.706396Z",
     "shell.execute_reply": "2023-09-18T11:36:31.703829Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.068869,
     "end_time": "2023-09-18T11:36:31.710571",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.641702",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG8 </td><td>5.706351</td><td>5.225881</td><td>4.103566</td><td>5.052406</td><td>4.946759</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG15</td><td>3.926754</td><td>4.401509</td><td>4.911829</td><td>5.128201</td><td>4.561287</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG18</td><td>5.897689</td><td>5.167584</td><td>4.563131</td><td>5.283955</td><td>5.677221</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG22</td><td>4.532928</td><td>3.096872</td><td>3.022277</td><td>2.616432</td><td>3.723506</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG28</td><td>3.543585</td><td>3.790763</td><td>4.133493</td><td>3.860960</td><td>4.145796</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG29</td><td>5.099636</td><td>4.409763</td><td>4.219668</td><td>5.049241</td><td>4.453881</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 6\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & vlogId & Extr & Agr & Cons & Emot & Open\\\\\n",
       "  & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG8  & 5.706351 & 5.225881 & 4.103566 & 5.052406 & 4.946759\\\\\n",
       "\t2 & VLOG15 & 3.926754 & 4.401509 & 4.911829 & 5.128201 & 4.561287\\\\\n",
       "\t3 & VLOG18 & 5.897689 & 5.167584 & 4.563131 & 5.283955 & 5.677221\\\\\n",
       "\t4 & VLOG22 & 4.532928 & 3.096872 & 3.022277 & 2.616432 & 3.723506\\\\\n",
       "\t5 & VLOG28 & 3.543585 & 3.790763 & 4.133493 & 3.860960 & 4.145796\\\\\n",
       "\t6 & VLOG29 & 5.099636 & 4.409763 & 4.219668 & 5.049241 & 4.453881\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 6\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 1 | VLOG8  | 5.706351 | 5.225881 | 4.103566 | 5.052406 | 4.946759 |\n",
       "| 2 | VLOG15 | 3.926754 | 4.401509 | 4.911829 | 5.128201 | 4.561287 |\n",
       "| 3 | VLOG18 | 5.897689 | 5.167584 | 4.563131 | 5.283955 | 5.677221 |\n",
       "| 4 | VLOG22 | 4.532928 | 3.096872 | 3.022277 | 2.616432 | 3.723506 |\n",
       "| 5 | VLOG28 | 3.543585 | 3.790763 | 4.133493 | 3.860960 | 4.145796 |\n",
       "| 6 | VLOG29 | 5.099636 | 4.409763 | 4.219668 | 5.049241 | 4.453881 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId Extr     Agr      Cons     Emot     Open    \n",
       "1 VLOG8  5.706351 5.225881 4.103566 5.052406 4.946759\n",
       "2 VLOG15 3.926754 4.401509 4.911829 5.128201 4.561287\n",
       "3 VLOG18 5.897689 5.167584 4.563131 5.283955 5.677221\n",
       "4 VLOG22 4.532928 3.096872 3.022277 2.616432 3.723506\n",
       "5 VLOG28 3.543585 3.790763 4.133493 3.860960 4.145796\n",
       "6 VLOG29 5.099636 4.409763 4.219668 5.049241 4.453881"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute output data frame\n",
    "testset_pred = \n",
    "    testset_vloggers[1] %>%\n",
    "    cbind(pred)\n",
    "\n",
    "head(testset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a1bb2",
   "metadata": {
    "_cell_guid": "4528893d-1930-4ccf-b4c0-51a0c773dae5",
    "_uuid": "2ef3a62f-62cf-42ca-9f50-c07a93d04ccf",
    "papermill": {
     "duration": 0.027381,
     "end_time": "2023-09-18T11:36:31.766159",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.738778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 Writing predictions to file\n",
    "\n",
    "You need to upload your predictions in .csv file. However, there are multiple columns: `Extr`, `Agr`, `Cons`, `Emot`, `Open`, while Kaggle expects **long format**!\n",
    "\n",
    "What does long format look like?\n",
    "\n",
    "- Every prediction on a single line.\n",
    "- Columns `vlogId` and `pers_axis` to map prediction *vlogger ID* and *personality axis*.\n",
    "\n",
    "To achieve this, `pivot_longer` to store the personality profile values into a single `value` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37d55f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:31.838231Z",
     "iopub.status.busy": "2023-09-18T11:36:31.836338Z",
     "iopub.status.idle": "2023-09-18T11:36:31.890383Z",
     "shell.execute_reply": "2023-09-18T11:36:31.888152Z"
    },
    "papermill": {
     "duration": 0.09609,
     "end_time": "2023-09-18T11:36:31.893490",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.797400",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>pers_axis</th><th scope=col>value</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG8 </td><td>Extr</td><td>5.706351</td></tr>\n",
       "\t<tr><td>VLOG8 </td><td>Agr </td><td>5.225881</td></tr>\n",
       "\t<tr><td>VLOG8 </td><td>Cons</td><td>4.103566</td></tr>\n",
       "\t<tr><td>VLOG8 </td><td>Emot</td><td>5.052406</td></tr>\n",
       "\t<tr><td>VLOG8 </td><td>Open</td><td>4.946759</td></tr>\n",
       "\t<tr><td>VLOG15</td><td>Extr</td><td>3.926754</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " vlogId & pers\\_axis & value\\\\\n",
       " <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG8  & Extr & 5.706351\\\\\n",
       "\t VLOG8  & Agr  & 5.225881\\\\\n",
       "\t VLOG8  & Cons & 4.103566\\\\\n",
       "\t VLOG8  & Emot & 5.052406\\\\\n",
       "\t VLOG8  & Open & 4.946759\\\\\n",
       "\t VLOG15 & Extr & 3.926754\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 3\n",
       "\n",
       "| vlogId &lt;chr&gt; | pers_axis &lt;chr&gt; | value &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| VLOG8  | Extr | 5.706351 |\n",
       "| VLOG8  | Agr  | 5.225881 |\n",
       "| VLOG8  | Cons | 4.103566 |\n",
       "| VLOG8  | Emot | 5.052406 |\n",
       "| VLOG8  | Open | 4.946759 |\n",
       "| VLOG15 | Extr | 3.926754 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId pers_axis value   \n",
       "1 VLOG8  Extr      5.706351\n",
       "2 VLOG8  Agr       5.225881\n",
       "3 VLOG8  Cons      4.103566\n",
       "4 VLOG8  Emot      5.052406\n",
       "5 VLOG8  Open      4.946759\n",
       "6 VLOG15 Extr      3.926754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>400</li><li>3</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400   3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset_pred_long <- testset_pred %>% \n",
    "    pivot_longer(c(Extr, Agr, Cons, Emot, Open), names_to = 'pers_axis')\n",
    "\n",
    "head(testset_pred_long)\n",
    "dim(testset_pred_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b28d68",
   "metadata": {
    "_cell_guid": "e62e1252-b466-424a-9edd-c6e71131dd95",
    "_uuid": "229b46a3-ff86-463d-ba3a-d91ce5e44b89",
    "papermill": {
     "duration": 0.028104,
     "end_time": "2023-09-18T11:36:31.949238",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.921134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "According to the competition's [Evaluation instructions](https://www.kaggle.com/competitions/bda-2023-bigfive/overview/evaluation), Kaggle expects file with two colums: `Id` and `Expected`.\n",
    "  \n",
    "The [Evaluation instructions](https://www.kaggle.com/competitions/bda-2023-bigfive/overview/evaluation) specifies we need to encode the `Agr` prediction for `VLOG8` as `VLOG8_Agr` in the `Id` column. To achieve this use `unite()` function of `dplyr`.\n",
    "\n",
    "`unite()` take:\n",
    "\n",
    "- a data frame as its first argument (implicitely passed by the piping operator `%>%`)\n",
    "- the name of new column as its second argument (`Id` below)\n",
    "- all extra arguments (`vlogId` and `pers_axis` below) are concatenated with an underscore in between\n",
    "\n",
    "Then write the resulting data frame to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab7d08f4",
   "metadata": {
    "_cell_guid": "84498e3c-77cc-4434-a91f-a099e72c93dd",
    "_uuid": "4a729773-ad88-4830-9c24-f0893a61d381",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:32.010386Z",
     "iopub.status.busy": "2023-09-18T11:36:32.008717Z",
     "iopub.status.idle": "2023-09-18T11:36:32.047508Z",
     "shell.execute_reply": "2023-09-18T11:36:32.045077Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.072885,
     "end_time": "2023-09-18T11:36:32.050655",
     "exception": false,
     "start_time": "2023-09-18T11:36:31.977770",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Id</th><th scope=col>Expected</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG8_Extr </td><td>5.706351</td></tr>\n",
       "\t<tr><td>VLOG8_Agr  </td><td>5.225881</td></tr>\n",
       "\t<tr><td>VLOG8_Cons </td><td>4.103566</td></tr>\n",
       "\t<tr><td>VLOG8_Emot </td><td>5.052406</td></tr>\n",
       "\t<tr><td>VLOG8_Open </td><td>4.946759</td></tr>\n",
       "\t<tr><td>VLOG15_Extr</td><td>3.926754</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Id & Expected\\\\\n",
       " <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG8\\_Extr  & 5.706351\\\\\n",
       "\t VLOG8\\_Agr   & 5.225881\\\\\n",
       "\t VLOG8\\_Cons  & 4.103566\\\\\n",
       "\t VLOG8\\_Emot  & 5.052406\\\\\n",
       "\t VLOG8\\_Open  & 4.946759\\\\\n",
       "\t VLOG15\\_Extr & 3.926754\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 2\n",
       "\n",
       "| Id &lt;chr&gt; | Expected &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| VLOG8_Extr  | 5.706351 |\n",
       "| VLOG8_Agr   | 5.225881 |\n",
       "| VLOG8_Cons  | 4.103566 |\n",
       "| VLOG8_Emot  | 5.052406 |\n",
       "| VLOG8_Open  | 4.946759 |\n",
       "| VLOG15_Extr | 3.926754 |\n",
       "\n"
      ],
      "text/plain": [
       "  Id          Expected\n",
       "1 VLOG8_Extr  5.706351\n",
       "2 VLOG8_Agr   5.225881\n",
       "3 VLOG8_Cons  4.103566\n",
       "4 VLOG8_Emot  5.052406\n",
       "5 VLOG8_Open  4.946759\n",
       "6 VLOG15_Extr 3.926754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the right format for Kaggle\n",
    "testset_pred_final <- testset_pred_long %>%\n",
    "    unite(Id, vlogId, pers_axis) %>%\n",
    "    rename(Expected = value)\n",
    "\n",
    "# Check if we succeeded\n",
    "head(testset_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fd358c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T11:36:32.110621Z",
     "iopub.status.busy": "2023-09-18T11:36:32.109037Z",
     "iopub.status.idle": "2023-09-18T11:36:32.149017Z",
     "shell.execute_reply": "2023-09-18T11:36:32.147177Z"
    },
    "papermill": {
     "duration": 0.072776,
     "end_time": "2023-09-18T11:36:32.151631",
     "exception": false,
     "start_time": "2023-09-18T11:36:32.078855",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'__MACOSX'</li><li>'__notebook__.ipynb'</li><li>'afinn.rds'</li><li>'imm6010.zip'</li><li>'NRC-Emotion-Lexicon'</li><li>'NRC-Emotion-Lexicon.zip'</li><li>'nrc.rds'</li><li>'predictions.csv'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '\\_\\_MACOSX'\n",
       "\\item '\\_\\_notebook\\_\\_.ipynb'\n",
       "\\item 'afinn.rds'\n",
       "\\item 'imm6010.zip'\n",
       "\\item 'NRC-Emotion-Lexicon'\n",
       "\\item 'NRC-Emotion-Lexicon.zip'\n",
       "\\item 'nrc.rds'\n",
       "\\item 'predictions.csv'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '__MACOSX'\n",
       "2. '__notebook__.ipynb'\n",
       "3. 'afinn.rds'\n",
       "4. 'imm6010.zip'\n",
       "5. 'NRC-Emotion-Lexicon'\n",
       "6. 'NRC-Emotion-Lexicon.zip'\n",
       "7. 'nrc.rds'\n",
       "8. 'predictions.csv'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"__MACOSX\"                \"__notebook__.ipynb\"     \n",
       "[3] \"afinn.rds\"               \"imm6010.zip\"            \n",
       "[5] \"NRC-Emotion-Lexicon\"     \"NRC-Emotion-Lexicon.zip\"\n",
       "[7] \"nrc.rds\"                 \"predictions.csv\"        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write to csv\n",
    "write_csv(testset_pred_final, file = \"predictions.csv\")\n",
    "\n",
    "# Check if the file was written successfully.\n",
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b735bf0",
   "metadata": {
    "_cell_guid": "d440c5a9-2554-4618-bdae-ae2625df13ef",
    "_uuid": "ceb46677-aa88-4b59-b6fe-1a6ea5afd432",
    "papermill": {
     "duration": 0.028663,
     "end_time": "2023-09-18T11:36:32.209648",
     "exception": false,
     "start_time": "2023-09-18T11:36:32.180985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Division of Labor\n",
    "\n",
    "Alex: adding extra text-based features, improving code readability, adding interaction effects and assesing their impact.\n",
    "\n",
    "Roy: adding NRC lexicon, running regression & stepwise selection, improving code readability and formatting as a whole.\n",
    "\n",
    "Titus: adding AFINN lexicon, adding most explanatory texts, searching for references, improving document presentation as a whole.\n",
    "\n",
    "Together: brainstorming ideas to improve our model, final revisions and proofreading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94330316",
   "metadata": {
    "papermill": {
     "duration": 0.028566,
     "end_time": "2023-09-18T11:36:32.267055",
     "exception": false,
     "start_time": "2023-09-18T11:36:32.238489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "Huntington, C. (n.d.). Big Five Personality Traits: Definition & Theory. Retrieved from https://www.berkeleywellbeing.com/big-five-personality-traits.html\n",
    "\n",
    "Schmitt, D. P., Realo, A., Voracek, M., & Allik, J. (2008). Why can't a man be more like a woman? Sex differences in Big Five personality traits across 55 cultures. Journal of Personality and Social Psychology, 94(1), 168–182. https://doi.org/10.1037/0022-3514.94.1.168\n",
    "\n",
    "Text Mining: Sentiment Analysis. (n.d.). Retrieved from https://afit-r.github.io/sentiment_analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 70.724264,
   "end_time": "2023-09-18T11:36:32.417418",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-18T11:35:21.693154",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
